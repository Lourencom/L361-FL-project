{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T01:10:28.014631Z",
     "iopub.status.busy": "2025-02-28T01:10:28.014446Z",
     "iopub.status.idle": "2025-02-28T01:10:28.032587Z",
     "shell.execute_reply": "2025-02-28T01:10:28.031785Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T01:10:28.036364Z",
     "iopub.status.busy": "2025-02-28T01:10:28.036170Z",
     "iopub.status.idle": "2025-02-28T01:10:32.729527Z",
     "shell.execute_reply": "2025-02-28T01:10:32.728747Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from logging import INFO, DEBUG\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.signal import medfilt\n",
    "from flwr.common import log, ndarrays_to_parameters\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from src.common.client_utils import (\n",
    "    load_femnist_dataset,\n",
    "    get_network_generator_cnn as get_network_generator,\n",
    "    get_device,\n",
    "    get_model_parameters,\n",
    "    aggregate_weighted_average,\n",
    ")\n",
    "\n",
    "\n",
    "from src.flwr_core import (\n",
    "    set_all_seeds,\n",
    "    get_paths,\n",
    "    decompress_dataset,\n",
    "    get_flower_client_generator,\n",
    "    sample_random_clients,\n",
    "    get_federated_evaluation_function,\n",
    ")\n",
    "\n",
    "from src.estimate import (\n",
    "    compute_critical_batch,\n",
    ")\n",
    "\n",
    "from src.experiments_simulation import (\n",
    "    run_simulation,\n",
    "    centralized_experiment,\n",
    ")\n",
    "\n",
    "from src.utils import get_centralized_acc_from_hist\n",
    "\n",
    "PathType = Path | str | None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T01:10:32.733820Z",
     "iopub.status.busy": "2025-02-28T01:10:32.733488Z",
     "iopub.status.idle": "2025-02-28T01:10:32.767846Z",
     "shell.execute_reply": "2025-02-28T01:10:32.766996Z"
    }
   },
   "outputs": [],
   "source": [
    "set_all_seeds()\n",
    "\n",
    "PATHS = get_paths()\n",
    "\n",
    "HOME_DIR = PATHS[\"home_dir\"]\n",
    "DATASET_DIR = PATHS[\"dataset_dir\"]\n",
    "DATA_DIR = PATHS[\"data_dir\"]\n",
    "CENTRALIZED_PARTITION = PATHS[\"centralized_partition\"]\n",
    "CENTRALIZED_MAPPING = PATHS[\"centralized_mapping\"]\n",
    "FEDERATED_PARTITION = PATHS[\"federated_partition\"]\n",
    "\n",
    "# extract dataset from tar.gz\n",
    "decompress_dataset(PATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T01:10:32.770851Z",
     "iopub.status.busy": "2025-02-28T01:10:32.770648Z",
     "iopub.status.idle": "2025-02-28T01:10:32.799242Z",
     "shell.execute_reply": "2025-02-28T01:10:32.798383Z"
    }
   },
   "outputs": [],
   "source": [
    "NETWORK_GENERATOR = get_network_generator()\n",
    "SEED_NET = NETWORK_GENERATOR()\n",
    "SEED_MODEL_PARAMS = get_model_parameters(SEED_NET)\n",
    "CID_CLIENT_GENERATOR = get_flower_client_generator(NETWORK_GENERATOR, FEDERATED_PARTITION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T01:10:32.803230Z",
     "iopub.status.busy": "2025-02-28T01:10:32.803015Z",
     "iopub.status.idle": "2025-02-28T01:10:37.781054Z",
     "shell.execute_reply": "2025-02-28T01:10:37.780067Z"
    }
   },
   "outputs": [],
   "source": [
    "# Centralized experiments\n",
    "centralized_experiment_batch_sizes = [32, 64, 128, 256, 512, 1024]\n",
    "\n",
    "# Load the centralized dataset using the same function as in FL.\n",
    "# The centralized mapping folder should be the one used in the FL centralized experiment.\n",
    "centralized_train_dataset = load_femnist_dataset(data_dir=DATA_DIR,mapping=CENTRALIZED_MAPPING, name=\"train\")\n",
    "centralized_test_dataset = load_femnist_dataset(data_dir=DATA_DIR, mapping=CENTRALIZED_MAPPING, name=\"test\")\n",
    "\n",
    "centralized_train_config = {\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 32,\n",
    "    \"client_learning_rate\": 0.01,\n",
    "    \"weight_decay\": 0,\n",
    "    \"num_workers\": 0,\n",
    "    \"max_batches\": 100,\n",
    "}\n",
    "\n",
    "centralized_test_config = {\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 0,\n",
    "    \"max_batches\": 100,\n",
    "    \"target_accuracy\": 0.60,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T01:10:37.785336Z",
     "iopub.status.busy": "2025-02-28T01:10:37.785176Z",
     "iopub.status.idle": "2025-02-28T01:10:38.362339Z",
     "shell.execute_reply": "2025-02-28T01:10:38.361848Z"
    }
   },
   "outputs": [],
   "source": [
    "# FL experiments\n",
    "experiment_batch_sizes = [16, 32, 64, 128, 256]\n",
    "cohort_sizes = [5, 10, 20, 50, 75, 100]\n",
    "\n",
    "\n",
    "# Federated configuration dictionary\n",
    "federated_train_config = {\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 32,\n",
    "    \"client_learning_rate\": 0.01,\n",
    "    \"weight_decay\": 0,\n",
    "    \"num_workers\": 0,\n",
    "    \"max_batches\": 100,\n",
    "    \"return_params\": False\n",
    "}\n",
    "\n",
    "federated_test_config: dict[str, Any] = {\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 0,\n",
    "    \"max_batches\": 100,\n",
    "}\n",
    "\n",
    "num_rounds = 10\n",
    "num_total_clients = 100\n",
    "num_evaluate_clients = 0\n",
    "num_clients_per_round = 10\n",
    "\n",
    "initial_parameters = ndarrays_to_parameters(SEED_MODEL_PARAMS)\n",
    "\n",
    "federated_evaluation_function = get_federated_evaluation_function(\n",
    "    batch_size=federated_test_config[\"batch_size\"],\n",
    "    num_workers=federated_test_config[\"num_workers\"],\n",
    "    model_generator=NETWORK_GENERATOR,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    max_batches=None if \"max_batches\" not in federated_test_config else federated_test_config[\"max_batches\"],\n",
    ")\n",
    "\n",
    "server_learning_rate = 1.0\n",
    "server_momentum = 0.0\n",
    "accept_failures = False\n",
    "\n",
    "\n",
    "CID_CLIENT_GENERATOR = get_flower_client_generator(NETWORK_GENERATOR, FEDERATED_PARTITION)\n",
    "\n",
    "list_of_ids = sample_random_clients(\n",
    "    num_total_clients, federated_train_config[\"batch_size\"],\n",
    "    CID_CLIENT_GENERATOR,\n",
    ")\n",
    "\n",
    "federated_client_generator = (\n",
    "    get_flower_client_generator(\n",
    "        NETWORK_GENERATOR, FEDERATED_PARTITION, lambda seq_id: list_of_ids[seq_id]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Experiments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR scaling\n",
    "\n",
    "Some rules usually are:\n",
    "\n",
    "**Linear scaling**\n",
    "\n",
    "$$ \\text{LR} \\propto \\text{batch size} $$\n",
    "\n",
    "so, starting with 0.1 at 256, we get then, 0.05 at 128, 0.025 at 64, 0.0125 at 32, 0.00625 at 16.\n",
    "\n",
    "However, this is not always the case, and it depends on the model, the dataset, the optimizer, etc.\n",
    "\n",
    "We can also use:\n",
    "\n",
    "**Sqrt scaling**\n",
    "\n",
    "$$ \\text{LR} \\propto \\sqrt{\\text{batch size}} $$\n",
    "\n",
    "Starting with 0.1 at 256, we get then, sqrt(128/256) * 0.1 at 128, sqrt(64/256) * 0.1 at 64, sqrt(32/256) * 0.1 at 32, sqrt(16/256) * 0.1 at 16.\n",
    "\n",
    "\n",
    "Or we can use **Learning Rate Finders**.\n",
    "\n",
    "***However***, in Federated Learning usually people use lower learning rates.\n",
    "\n",
    "While in centralized learning its normal to use LR = 0.1 to batch size = 256, in federated learning its usual to use 0.005-0.01 for batch sizes 16-32.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Centralized run with varying batch sizes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T01:10:38.364289Z",
     "iopub.status.busy": "2025-02-28T01:10:38.364136Z",
     "iopub.status.idle": "2025-02-28T01:14:30.783069Z",
     "shell.execute_reply": "2025-02-28T01:14:30.781888Z"
    }
   },
   "outputs": [],
   "source": [
    "centralized_experiment_results = []\n",
    "\n",
    "for batch_size in centralized_experiment_batch_sizes:\n",
    "\n",
    "    train_cfg = centralized_train_config.copy()\n",
    "    train_cfg[\"batch_size\"] = batch_size\n",
    "    ratio = np.sqrt(batch_size / 256)\n",
    "    train_cfg[\"learning_rate\"] = ratio * 0.1\n",
    "\n",
    "    test_cfg = centralized_test_config.copy()\n",
    "    test_cfg[\"batch_size\"] = batch_size\n",
    "\n",
    "    # Create DataLoaders with the same settings.\n",
    "    centralized_train_loader = DataLoader(\n",
    "        dataset=centralized_train_dataset,\n",
    "        batch_size=train_cfg[\"batch_size\"],\n",
    "        shuffle=True,                # Shuffle for training\n",
    "        num_workers=train_cfg[\"num_workers\"],\n",
    "        drop_last=True,              # If FL training drops last batch, do the same here.\n",
    "    )\n",
    "\n",
    "    centralized_test_loader = DataLoader(\n",
    "        dataset=centralized_test_dataset,\n",
    "        batch_size=test_cfg[\"batch_size\"],\n",
    "        shuffle=False,               # No shuffling during evaluation\n",
    "        num_workers=test_cfg[\"num_workers\"],\n",
    "        drop_last=False,\n",
    "    )\n",
    "    \n",
    "    centralized_results = centralized_experiment(centralized_train_config, centralized_test_config, centralized_train_loader, centralized_test_loader, get_device(), NETWORK_GENERATOR())\n",
    "    centralized_experiment_results.append((batch_size, centralized_results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T01:14:30.785693Z",
     "iopub.status.busy": "2025-02-28T01:14:30.785466Z",
     "iopub.status.idle": "2025-02-28T01:14:31.111728Z",
     "shell.execute_reply": "2025-02-28T01:14:31.110901Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_experiment(save_file_name):\n",
    "    with open(save_file_name, \"r\") as f:\n",
    "        results_dict = json.load(f)\n",
    "    return results_dict\n",
    "\n",
    "def save_experiment(save_file_name, results_dict):\n",
    "    if os.path.exists(save_file_name):\n",
    "        print(f\"File {save_file_name} already exists\")\n",
    "        return\n",
    "    with open(save_file_name, \"w\") as f:\n",
    "        json.dump(results_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T01:14:31.115337Z",
     "iopub.status.busy": "2025-02-28T01:14:31.114958Z",
     "iopub.status.idle": "2025-02-28T01:14:31.148126Z",
     "shell.execute_reply": "2025-02-28T01:14:31.147500Z"
    }
   },
   "outputs": [],
   "source": [
    "for batch_size, results in centralized_experiment_results:\n",
    "    save_experiment(f\"centralized_experiment_results_{batch_size}.json\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Their plot:\n",
    "\n",
    "Y-axis: $\\epsilon_\\text{opt}(B) / \\epsilon_\\text{max}$, which is $\\frac{1}{1+\\Beta_{noise}/B}$, B batch size, $\\Beta_{noise}$ is the noise scale.\n",
    "\n",
    "X-axis: $\\frac{B}{\\Beta_{noise}}$, B batch size, $\\Beta_{noise}$ is noise scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T01:14:31.151881Z",
     "iopub.status.busy": "2025-02-28T01:14:31.151744Z",
     "iopub.status.idle": "2025-02-28T01:14:31.578777Z",
     "shell.execute_reply": "2025-02-28T01:14:31.578128Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assume centralized_experiment_results is a list of tuples:\n",
    "# (batch_size, results)\n",
    "# where results is a dict containing:\n",
    "#   - \"compute_budgets\": samples per epoch\n",
    "#   - \"training_time\": time per epoch\n",
    "#   - \"noise_scales\": list of noise scale values per epoch\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "centralized_experiment_results = [\n",
    "    (batch_size, load_experiment(f\"centralized_experiment_results_{batch_size}.json\"))\n",
    "    for batch_size in centralized_experiment_batch_sizes\n",
    "]\n",
    "# Left subplot: Compute Budget vs. Cumulative Training Time for each batch size\n",
    "for batch_size, results in centralized_experiment_results:\n",
    "    # Calculate cumulative training time (sum over epochs)\n",
    "    cumulative_time = np.sum(results[\"training_time\"])\n",
    "    compute_budget = np.sum(results[\"compute_cost\"])\n",
    "    axes[0].plot(cumulative_time, compute_budget, marker='o', label=f\"Batch size: {batch_size}\")\n",
    "    \n",
    "    log(INFO, f\"Batch size: {batch_size}\")\n",
    "    log(INFO, f\"Total Training Time (s): {cumulative_time}\")\n",
    "    log(INFO, f\"Compute Budget (samples): {compute_budget}\")\n",
    "\n",
    "axes[0].set_xlabel(\"Total Training Time (s)\")\n",
    "axes[0].set_ylabel(\"Compute Budget (Total Samples Processed)\")\n",
    "axes[0].set_title(\"Compute Budget vs. Total Training Time\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Right subplot: Noise Scale vs. Cumulative Training Time for each batch size\n",
    "for batch_size, results in centralized_experiment_results:\n",
    "    noise_scale = np.mean(results[\"noise_scales\"]) # do we average ??\n",
    "\n",
    "    x_axis = batch_size / noise_scale\n",
    "    y_axis = 1 / (1 + (noise_scale / batch_size))\n",
    "    \n",
    "    axes[1].plot(x_axis, y_axis, marker='o', label=f\"Batch size: {batch_size}\")\n",
    "    \n",
    "    log(INFO, f\"Batch size: {batch_size}\")\n",
    "    log(INFO, f\"Cumulative Training Time (s): {cumulative_time}\")\n",
    "    log(INFO, f\"Noise Scale: {noise_scale}\")\n",
    "\n",
    "\n",
    "axes[1].set_xlabel(\"Batch Size / Noise Scale\")\n",
    "axes[1].set_ylabel(fr\"${{\\epsilon_\\text{{B}}}} / {{\\epsilon_\\text{{max}}}}$\")\n",
    "axes[1].set_title(\"Predicted Training Speed\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also observe that increasing batch size appears does in fact alter performance.\n",
    "\n",
    "Furthermore, both $\\beta_{\\text{simple}}$ computation and the empirical results seem to indicate optimal batch sizes with magnitudes in the hundreds (100-1000)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critical batch size estimation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_keys = ['training_time', 'samples_processed', 'noise_scale', 'train_loss', 'actual_batches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_simples = []\n",
    "results = []\n",
    "batch_sizes = [16, 32, 64, 128, 256]\n",
    "for batch_size in batch_sizes:\n",
    "    train_cfg = federated_train_config.copy()\n",
    "    train_cfg[\"batch_size\"] = batch_size\n",
    "    ratio = np.sqrt(batch_size / 256)\n",
    "    train_cfg[\"client_learning_rate\"] = ratio * 0.01 # Same as centralized, but should be lower for FL\n",
    "    train_cfg[\"return_params\"] = True # needed to get g locals for estimation\n",
    "\n",
    "    test_cfg = federated_test_config.copy()\n",
    "    test_cfg[\"batch_size\"] = batch_size\n",
    "\n",
    "    local_list_of_ids = sample_random_clients(num_total_clients, train_cfg[\"batch_size\"], CID_CLIENT_GENERATOR)\n",
    "    local_federated_client_generator = get_flower_client_generator(NETWORK_GENERATOR, FEDERATED_PARTITION, lambda seq_id: local_list_of_ids[seq_id])\n",
    "\n",
    "    parameters_for_each_round, hist = run_simulation(\n",
    "        num_rounds = 10,\n",
    "        num_total_clients = num_total_clients,\n",
    "        num_clients_per_round = num_clients_per_round,\n",
    "        num_evaluate_clients = num_evaluate_clients,\n",
    "        min_available_clients = num_total_clients,\n",
    "        min_fit_clients = num_clients_per_round,\n",
    "        min_evaluate_clients = num_evaluate_clients,\n",
    "        evaluate_fn = federated_evaluation_function,\n",
    "        on_fit_config_fn = lambda _: train_cfg,\n",
    "        on_evaluate_config_fn = lambda _: test_cfg,\n",
    "        initial_parameters = initial_parameters,\n",
    "        fit_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        evaluate_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        federated_client_generator = local_federated_client_generator,\n",
    "        server_learning_rate=server_learning_rate,\n",
    "        server_momentum=server_momentum,\n",
    "        accept_failures=accept_failures,\n",
    "        target_accuracy=0.60,\n",
    "        use_target_accuracy=True\n",
    "        )\n",
    "    n_params = len(hist.metrics_distributed_fit.keys()) - 5\n",
    "    param_keys = list(set(hist.metrics_distributed_fit.keys()) - set(metric_keys))\n",
    "    hist_metrics = {key: hist.metrics_distributed_fit[key] for key in metric_keys}\n",
    "    params = [hist.metrics_distributed_fit[key] for key in param_keys]\n",
    "    del hist\n",
    "    #gc.collect()\n",
    "\n",
    "    res = (batch_size, parameters_for_each_round, hist_metrics, params)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "B_simples = [] # 244230\n",
    "n_clients = 10\n",
    "\n",
    "K = n_clients\n",
    "alpha = 0.9\n",
    "\n",
    "for k, res in enumerate(results):\n",
    "    batch_size, parameters_for_each_round, hist_metrics, params = res\n",
    "    B_small = batch_size\n",
    "    B_big = num_clients_per_round * batch_size\n",
    "    G_local = params\n",
    "    n_rounds = len(params[0])\n",
    "    params_filt = [params[i] for i in range(len(params)) if len(params[i]) == n_rounds]\n",
    "    G_local_by_rounds = [[params_filt[i][j][1]['all'] for i in range(len(params_filt))] for j in range(n_rounds)]\n",
    "    B_simples.append([0] * n_rounds)\n",
    "    for round_idx, G_local in enumerate(G_local_by_rounds):\n",
    "        G_local = [[el[1] for el in G_loc] for G_loc in G_local]\n",
    "        G_local_filt = [G_local[i] for i in range(len(G_local)) if len(G_local[i]) == 10]\n",
    "        G_local_filt = np.array(G_local_filt)\n",
    "        G_local_filt = G_local_filt.reshape(K, -1)\n",
    "\n",
    "        G_local_filt = [torch.tensor(G_local) for G_local in G_local_filt]\n",
    "        \n",
    "        local_norm_squared = torch.tensor([torch.norm(G_local)**2 for G_local in G_local_filt])\n",
    "\n",
    "        GBsmall_squared = local_norm_squared.sum() / K\n",
    "\n",
    "        G_big = sum(G_local_filt) / K\n",
    "\n",
    "        GBbig_squared = torch.norm(G_big)**2 \n",
    "\n",
    "        G2 = (1 / (B_big - B_small)) * (B_big * GBbig_squared - B_small * GBsmall_squared) \n",
    "\n",
    "        S = (B_small * B_big / (B_big - B_small)) * (GBbig_squared - GBsmall_squared)\n",
    "\n",
    "        B_simple = S/G2\n",
    "\n",
    "        B_simples[k][round_idx] = B_simple\n",
    "\n",
    "B_simples = [[el.item() for el in B_simple] for B_simple in B_simples]\n",
    "print(B_simples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_simples =  [[-11687.922491437468, 50476.83433348289, -8360.412075212149, 17319.119343169154, -25954.21660271833, 23370.69120739089, -4962.304179443327, -171059.17398979756], [-19385.786387690354, -48296.90692407105, 14661.483253924791, -13137.487134913083, -78752.06911183994, -43549.34807501476, 33191.38345639109, -8916.155326480151, 26379.271823599152, -17644.262975900085, 56942.85065950044], [63790.0779072847, 18194.037770084386, 20838.652107947135, 67274.89421292844, -313061.2900299799, -271432.3756642664, -63409.341255032, 100693.78565294151, -405953.26435536693, -57494.052391349855, -20407.116358130268], [0,0,0,0,0], [136404.8146034731, 382788.4590252541, -303602.1198341131, -391914.46840772225, -100359.90833068614, -222389.64401717987, -67227.1535142801, 99115.03185441485, -141411.7204019892, 67098.77896076355, -87909.66401102718, 673764.565140841, -89414.50505702938, -128119.07932215642, -81017.08955323228, 168047.6320547225, -570472.3537002297, -27778430.098110802, 228947.00354616542, -91905.22217824718, -143878.5904577031, 428230.9802809606, -65157.41155478921, -81947.86215890732, -50379.05382373369, -114053.67299065265, -100041.06682634717, -62604.96306232306, -257614.42012444683, -47455.47731560237, 227508.80388985266, 120620.7368087462, -68318.8873917895, -85052.34328022464, 92665.1515040749, -324533.92759395094, -456602.2352201042, -886315.7575465435, 793226.2717526885, 346927.0548617065, -219646.0704645006, 409218.6427205505, 312957.09194129705, -217288.8919829346, -230479.10911932818, -141406.95367459086, 188017.85735417364]]\n",
    "B_simples = [[abs(el) for el in subl] for subl in B_simples]\n",
    "# apply median filter for each sublist\n",
    "B_simple_median = [medfilt(subl, 5) for subl in B_simples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot nicely B_simple through the rounds\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for bs, B_sim in zip(batch_sizes, B_simple_median):\n",
    "    ax.plot(B_sim, label=f\"Batch size: {bs}\")\n",
    "ax.set_xlabel('Round')\n",
    "ax.set_ylabel('Batch Size')\n",
    "ax.set_title('Critical Batch Size through Rounds')\n",
    "ax.legend()\n",
    "# log scale y\n",
    "ax.set_yscale('log')\n",
    "#ax.set_ylim(0, 5000)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **FL run with varying batch sizes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T01:14:31.583120Z",
     "iopub.status.busy": "2025-02-28T01:14:31.582654Z",
     "iopub.status.idle": "2025-02-28T01:25:50.692299Z",
     "shell.execute_reply": "2025-02-28T01:25:50.691630Z"
    }
   },
   "outputs": [],
   "source": [
    "total_batch_results = []\n",
    "\n",
    "for batch_size in experiment_batch_sizes:\n",
    "    train_cfg = federated_train_config.copy()\n",
    "    train_cfg[\"batch_size\"] = batch_size\n",
    "    ratio = np.sqrt(batch_size / 256)\n",
    "    train_cfg[\"client_learning_rate\"] = ratio * 0.01 # Same as centralized, but should be lower for FL\n",
    "\n",
    "    test_cfg = federated_test_config.copy()\n",
    "    test_cfg[\"batch_size\"] = batch_size\n",
    "\n",
    "    local_list_of_ids = sample_random_clients(num_total_clients, train_cfg[\"batch_size\"], CID_CLIENT_GENERATOR)\n",
    "    local_federated_client_generator = get_flower_client_generator(NETWORK_GENERATOR, FEDERATED_PARTITION, lambda seq_id: local_list_of_ids[seq_id])\n",
    "\n",
    "    parameters_for_each_round, hist = run_simulation(\n",
    "        num_rounds = num_rounds,\n",
    "        num_total_clients = num_total_clients,\n",
    "        num_clients_per_round = num_clients_per_round,\n",
    "        num_evaluate_clients = num_evaluate_clients,\n",
    "        min_available_clients = num_total_clients,\n",
    "        min_fit_clients = num_clients_per_round,\n",
    "        min_evaluate_clients = num_evaluate_clients,\n",
    "        evaluate_fn = federated_evaluation_function,\n",
    "        on_fit_config_fn = lambda _: train_cfg,\n",
    "        on_evaluate_config_fn = lambda _: test_cfg,\n",
    "        initial_parameters = initial_parameters,\n",
    "        fit_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        evaluate_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        federated_client_generator = local_federated_client_generator,\n",
    "        server_learning_rate=server_learning_rate,\n",
    "        server_momentum=server_momentum,\n",
    "        accept_failures=accept_failures,\n",
    "        target_accuracy=0.60,\n",
    "        use_target_accuracy=True,\n",
    "        )\n",
    "\n",
    "    total_batch_results.append((batch_size, parameters_for_each_round, hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T01:25:50.696769Z",
     "iopub.status.busy": "2025-02-28T01:25:50.696591Z",
     "iopub.status.idle": "2025-02-28T01:25:51.059515Z",
     "shell.execute_reply": "2025-02-28T01:25:51.058877Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_experiment(save_file_name, batch_size, parameters_for_each_round, hist):\n",
    "    \"\"\"Save experiment results using pickle.\n",
    "    \n",
    "    Args:\n",
    "        save_file_name (str): Path to save the results\n",
    "        batch_size (int): Batch size used in experiment\n",
    "        parameters_for_each_round (list): List of model parameters for each round\n",
    "        hist (History): Flower History object containing metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    results_dict = {\n",
    "        'batch_size': batch_size,\n",
    "        'parameters_for_each_round': parameters_for_each_round,\n",
    "        'history': hist\n",
    "    }\n",
    "    \n",
    "    with open(save_file_name, 'wb') as f:  # Note: 'wb' for binary write mode\n",
    "        pickle.dump(results_dict, f)\n",
    "\n",
    "def load_experiment(file_name):\n",
    "    \"\"\"Load experiment results from a pickle file.\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Path to the results file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (batch_size, parameters_for_each_round, hist)\n",
    "    \"\"\"\n",
    "    with open(file_name, 'rb') as f:  # Note: 'rb' for binary read mode\n",
    "        results_dict = pickle.load(f)\n",
    "    \n",
    "    return (\n",
    "        results_dict['batch_size'],\n",
    "        results_dict['parameters_for_each_round'],\n",
    "        results_dict['history'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T01:25:51.062781Z",
     "iopub.status.busy": "2025-02-28T01:25:51.062633Z",
     "iopub.status.idle": "2025-02-28T01:25:51.119684Z",
     "shell.execute_reply": "2025-02-28T01:25:51.119226Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for batch_size, parameters, hist in total_batch_results:\n",
    "    save_experiment(f\"federated_batch_results_{batch_size}.pkl\", batch_size, parameters, hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load experiments\n",
    "batch_sizes = [16, 32, 64, 128, 256]\n",
    "total_batch_results = []\n",
    "for batch_size in batch_sizes:\n",
    "    total_batch_results.append(load_experiment(f\"results/federated_batch_results_{batch_size}.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T01:25:51.123754Z",
     "iopub.status.busy": "2025-02-28T01:25:51.123593Z",
     "iopub.status.idle": "2025-02-28T01:25:51.462428Z",
     "shell.execute_reply": "2025-02-28T01:25:51.461932Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "# Bottom-left: Compute Budget vs Training Time\n",
    "for batch_size, params, hist in total_batch_results:\n",
    "    times = []\n",
    "    samples = []\n",
    "    for round_idx, round_metrics in hist.metrics_distributed_fit['training_time']:\n",
    "        round_times = [t for _, t in round_metrics['all']]\n",
    "        times.append(np.mean(round_times))\n",
    "        \n",
    "    for round_idx, round_metrics in hist.metrics_distributed_fit['samples_processed']:\n",
    "        round_samples = [s for _, s in round_metrics['all']]\n",
    "        samples.append(np.sum(round_samples))\n",
    "    \n",
    "    cumulative_time = np.sum(times)\n",
    "    total_samples = np.sum(samples)\n",
    "    axes[0].plot(cumulative_time, total_samples, marker='o', label=f\"Local batch size: {batch_size}\")\n",
    "\n",
    "axes[0].set_xlabel(\"Total Training Time (s)\")\n",
    "axes[0].set_ylabel(\"Compute Budget (Total Samples Processed)\")\n",
    "axes[0].set_title(\"Compute Budget vs. Total Training Time\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Bottom-right: Noise Scale Analysis\n",
    "for batch_size, params, hist in total_batch_results:\n",
    "    noise_scales = []\n",
    "    for round_idx, round_metrics in hist.metrics_distributed_fit['noise_scale']:\n",
    "        round_noise_scales = [ns for _, ns in round_metrics['all']]\n",
    "        noise_scale = np.mean(round_noise_scales)\n",
    "        noise_scales.append(noise_scale)\n",
    "    \n",
    "    avg_noise_scale = np.mean(noise_scales)\n",
    "    x_axis = batch_size / (avg_noise_scale + 1e-10)\n",
    "    y_axis = 1 / (1 + (avg_noise_scale / batch_size))\n",
    "    \n",
    "    axes[1].plot(x_axis, y_axis, marker='o', label=f\"Batch size: {batch_size}\")\n",
    "\n",
    "axes[1].set_xlabel(\"Batch Size / Noise Scale\")\n",
    "axes[1].set_ylabel(fr\"${{\\epsilon_\\text{{B}}}} / {{\\epsilon_\\text{{max}}}}$\")\n",
    "axes[1].set_title(\"Predicted Training Speed\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T01:25:51.466298Z",
     "iopub.status.busy": "2025-02-28T01:25:51.466148Z",
     "iopub.status.idle": "2025-02-28T01:25:51.490098Z",
     "shell.execute_reply": "2025-02-28T01:25:51.489636Z"
    }
   },
   "outputs": [],
   "source": [
    "for batch_size, params, hist in total_batch_results:\n",
    "    print(\"Batch size: \", batch_size)\n",
    "\n",
    "    noise_scales = hist.metrics_distributed_fit['noise_scale']\n",
    "\n",
    "    crit_batches = []\n",
    "    for round, round_noise_scales in noise_scales:\n",
    "        actual_noise_scales = [val for _, val in round_noise_scales['all']]\n",
    "        crit_batch = compute_critical_batch(actual_noise_scales, .001)\n",
    "        print(\"---\", crit_batch)\n",
    "        crit_batches.append(crit_batch)\n",
    "    print(np.mean(crit_batches)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running FL experiments with multiple batch sizes, we do observe that there must be something as \"critical\" batch size.\n",
    "\n",
    "However, we must investigate how to identify it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Running FL with varying cohort sizes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating average time per round\n",
    "\n",
    "Since Flower is simulating multiple clients using multiple threads, a problem appears where the time we measure in multi-client setup is influenced by the context switching between threads and the waiting time.\n",
    "\n",
    "Therefore, we derive an assumption that the time per round is constant when we fix a batch size (which we do in this experiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_size = 2\n",
    "train_cfg = federated_train_config.copy()\n",
    "ratio = np.sqrt(cohort_size / 100)\n",
    "train_cfg[\"client_learning_rate\"] = ratio * 0.01\n",
    "#train_cfg[\"max_batches\"] = 1000\n",
    "\n",
    "test_cfg = federated_test_config.copy()\n",
    "\n",
    "parameters_for_each_round, hist = run_simulation(\n",
    "    num_rounds = 1,\n",
    "    num_total_clients = num_total_clients,\n",
    "    num_clients_per_round = cohort_size,\n",
    "    num_evaluate_clients = num_evaluate_clients,\n",
    "    min_available_clients = num_total_clients,\n",
    "    min_fit_clients = cohort_size,\n",
    "    min_evaluate_clients = num_evaluate_clients,\n",
    "    evaluate_fn = federated_evaluation_function,\n",
    "    on_fit_config_fn = lambda _: train_cfg,\n",
    "    on_evaluate_config_fn = lambda _: test_cfg,\n",
    "    initial_parameters = initial_parameters,\n",
    "    fit_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "    evaluate_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "    federated_client_generator = federated_client_generator,\n",
    "    server_learning_rate=server_learning_rate,\n",
    "    server_momentum=server_momentum,\n",
    "    accept_failures=accept_failures,\n",
    "    target_accuracy=0.60,\n",
    "    use_target_accuracy=True,\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_times_per_round = []\n",
    "for (round_idx, round_metrics), metrics_acc in zip(hist.metrics_distributed_fit['training_time'], hist.metrics_centralized['accuracy']):\n",
    "    round_times = [t for _, t in round_metrics['all']]\n",
    "    avg_time_per_round = np.mean(round_times)    \n",
    "    avg_times_per_round.append(avg_time_per_round)\n",
    "\n",
    "time_per_round = np.median(avg_times_per_round)\n",
    "print(time_per_round) # 0.00427"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T01:25:51.493063Z",
     "iopub.status.busy": "2025-02-28T01:25:51.492908Z",
     "iopub.status.idle": "2025-02-28T01:59:17.593838Z",
     "shell.execute_reply": "2025-02-28T01:59:17.593098Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_experiment(save_file_name, batch_size, parameters_for_each_round, hist):\n",
    "    \"\"\"Save experiment results using pickle.\n",
    "    \n",
    "    Args:\n",
    "        save_file_name (str): Path to save the results\n",
    "        batch_size (int): Batch size used in experiment\n",
    "        parameters_for_each_round (list): List of model parameters for each round\n",
    "        hist (History): Flower History object containing metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    results_dict = {\n",
    "        'batch_size': batch_size,\n",
    "        'parameters_for_each_round': parameters_for_each_round,\n",
    "        'history': hist\n",
    "    }\n",
    "    \n",
    "    with open(save_file_name, 'wb') as f:  # Note: 'wb' for binary write mode\n",
    "        pickle.dump(results_dict, f)\n",
    "\n",
    "def load_experiment(file_name):\n",
    "    \"\"\"Load experiment results from a pickle file.\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Path to the results file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (batch_size, parameters_for_each_round, hist)\n",
    "    \"\"\"\n",
    "    with open(file_name, 'rb') as f:  # Note: 'rb' for binary read mode\n",
    "        results_dict = pickle.load(f)\n",
    "    \n",
    "    return (\n",
    "        results_dict['batch_size'],\n",
    "        results_dict['parameters_for_each_round'],\n",
    "        results_dict['history'],\n",
    "    )\n",
    "\n",
    "total_cohort_results = []\n",
    "cohort_sizes =  [ 5, 10, 20, 50, 75, 100]\n",
    "for cohort_size in cohort_sizes:\n",
    "    train_cfg = federated_train_config.copy()\n",
    "    ratio = np.sqrt(cohort_size / 100)\n",
    "    train_cfg[\"client_learning_rate\"] = ratio * 0.01\n",
    "    #train_cfg[\"max_batches\"] = 1000\n",
    "\n",
    "    test_cfg = federated_test_config.copy()\n",
    "\n",
    "    parameters_for_each_round, hist = run_simulation(\n",
    "        num_rounds = 100,\n",
    "        num_total_clients = num_total_clients,\n",
    "        num_clients_per_round = cohort_size,\n",
    "        num_evaluate_clients = num_evaluate_clients,\n",
    "        min_available_clients = num_total_clients,\n",
    "        min_fit_clients = cohort_size,\n",
    "        min_evaluate_clients = num_evaluate_clients,\n",
    "        evaluate_fn = federated_evaluation_function,\n",
    "        on_fit_config_fn = lambda _: train_cfg,\n",
    "        on_evaluate_config_fn = lambda _: test_cfg,\n",
    "        initial_parameters = initial_parameters,\n",
    "        fit_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        evaluate_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        federated_client_generator = federated_client_generator,\n",
    "        server_learning_rate=server_learning_rate,\n",
    "        server_momentum=server_momentum,\n",
    "        accept_failures=accept_failures,\n",
    "        target_accuracy=0.60,\n",
    "        use_target_accuracy=True,\n",
    "        )\n",
    "\n",
    "    total_cohort_results.append((cohort_size, parameters_for_each_round, hist))\n",
    "    save_experiment(f\"results/federated_cohort_results1_{cohort_size}.pkl\", cohort_size, parameters_for_each_round=parameters_for_each_round, hist=hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort_size, parameters, hist in total_cohort_results:\n",
    "    save_experiment(f\"results/federated_cohort_results1_{cohort_size}.pkl\", cohort_size, parameters_for_each_round=parameters, hist=hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_sizes =  [5, 10, 20, 50, 75, 100]\n",
    "total_cohort_results = []\n",
    "for cohort_size in cohort_sizes:\n",
    "    total_cohort_results.append(load_experiment(f\"results/federated_cohort_results_{cohort_size}.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort_size, params, hist in total_cohort_results:\n",
    "    print(f\"COHORT SIZE: {cohort_size}\")\n",
    "    print(\"--------------------------------\")\n",
    "    for (round_idx, round_metrics), metrics_acc in zip(hist.metrics_distributed_fit['training_time'], hist.metrics_centralized['accuracy']):\n",
    "        round_times = [t for _, t in round_metrics['all']]\n",
    "        print(f\"ROUND {round_idx}\")\n",
    "        print(f\"TRAINING TIME: {round_times}, ACC: {metrics_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T01:59:17.598095Z",
     "iopub.status.busy": "2025-02-28T01:59:17.597879Z",
     "iopub.status.idle": "2025-02-28T01:59:18.412340Z",
     "shell.execute_reply": "2025-02-28T01:59:18.411754Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "# Bottom-left: Compute Budget vs Training Time\n",
    "for cohort_size, params, hist in total_cohort_results:\n",
    "    times = []\n",
    "    samples = []\n",
    "    num_rounds = len(hist.metrics_distributed_fit['samples_processed'])\n",
    "    print(num_rounds)\n",
    "    cumulative_time = num_rounds * time_per_round\n",
    "\n",
    "    for round_idx, round_metrics in hist.metrics_distributed_fit['samples_processed']:\n",
    "        round_samples = [s for _, s in round_metrics['all']]\n",
    "        samples.append(np.sum(round_samples))\n",
    "    \n",
    "    \n",
    "    total_samples = np.sum(samples)\n",
    "    axes[0].plot(cumulative_time, total_samples, marker='o', label=f\"Cohort size: {cohort_size}\")\n",
    "\n",
    "axes[0].set_xlabel(\"Total Training Time (s)\")\n",
    "axes[0].set_ylabel(\"Compute Budget (Total Samples Processed)\")\n",
    "axes[0].set_title(\"Compute Budget vs. Total Training Time\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Bottom-right: Noise Scale Analysis\n",
    "for cohort_size, params, hist in total_cohort_results:\n",
    "    noise_scales = []\n",
    "    for round_idx, round_metrics in hist.metrics_distributed_fit['noise_scale']:\n",
    "        round_noise_scales = [ns for _, ns in round_metrics['all']]\n",
    "        noise_scale = np.mean(round_noise_scales)\n",
    "        noise_scales.append(noise_scale)\n",
    "    \n",
    "    avg_noise_scale = np.mean(noise_scales)\n",
    "    x_axis = cohort_size / (avg_noise_scale + 1e-10)\n",
    "    y_axis = 1 / (1 + (avg_noise_scale / cohort_size))\n",
    "    \n",
    "    axes[1].plot(x_axis, y_axis, marker='o', label=f\"Cohort size: {cohort_size}\")\n",
    "\n",
    "axes[1].set_xlabel(\"Cohort Size / Noise Scale\")\n",
    "axes[1].set_ylabel(fr\"${{\\epsilon_\\text{{B}}}} / {{\\epsilon_\\text{{max}}}}$\")\n",
    "axes[1].set_title(\"Predicted Training Speed\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global batch size vs time per round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#16 - 256\n",
    "# 5 - 100\n",
    "\n",
    "\n",
    "# 80 - 25000\n",
    "\n",
    "# 100 = 20bs * 5c\n",
    "# 1000 = 50bs * 20cs\n",
    "# 10000 = 200bs * 50cs\n",
    "# 25000 = 250bs * 100cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_per_round = []\n",
    "for batch_size in [20, 50, 200, 250]:\n",
    "    cohort_size = 2\n",
    "    train_cfg = federated_train_config.copy()\n",
    "    ratio = np.sqrt(cohort_size / 100)\n",
    "    train_cfg[\"client_learning_rate\"] = ratio * 0.01\n",
    "    #train_cfg[\"max_batches\"] = 1000\n",
    "\n",
    "    test_cfg = federated_test_config.copy()\n",
    "\n",
    "    parameters_for_each_round, hist = run_simulation(\n",
    "        num_rounds = 1,\n",
    "        num_total_clients = num_total_clients,\n",
    "        num_clients_per_round = cohort_size,\n",
    "        num_evaluate_clients = num_evaluate_clients,\n",
    "        min_available_clients = num_total_clients,\n",
    "        min_fit_clients = cohort_size,\n",
    "        min_evaluate_clients = num_evaluate_clients,\n",
    "        evaluate_fn = federated_evaluation_function,\n",
    "        on_fit_config_fn = lambda _: train_cfg,\n",
    "        on_evaluate_config_fn = lambda _: test_cfg,\n",
    "        initial_parameters = initial_parameters,\n",
    "        fit_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        evaluate_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        federated_client_generator = federated_client_generator,\n",
    "        server_learning_rate=server_learning_rate,\n",
    "        server_momentum=server_momentum,\n",
    "        accept_failures=accept_failures,\n",
    "        target_accuracy=0.60,\n",
    "        use_target_accuracy=True,\n",
    "        )\n",
    "\n",
    "    avg_times_per_round = []\n",
    "    for (round_idx, round_metrics), metrics_acc in zip(hist.metrics_distributed_fit['training_time'], hist.metrics_centralized['accuracy']):\n",
    "        round_times = [t for _, t in round_metrics['all']]\n",
    "        avg_time_per_round = np.mean(round_times)    \n",
    "        avg_times_per_round.append(avg_time_per_round)\n",
    "\n",
    "    time_per_round = np.median(avg_times_per_round)\n",
    "    print(time_per_round) # 0.00427\n",
    "    times_per_round.append(time_per_round)\n",
    "\n",
    "print(times_per_round) # [0.003793160191070807, 0.004481774148126263, 0.004956590107499679, 0.004278853300000662]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_experiment(save_file_name, batch_size, parameters_for_each_round, hist):\n",
    "    \"\"\"Save experiment results using pickle.\n",
    "    \n",
    "    Args:\n",
    "        save_file_name (str): Path to save the results\n",
    "        batch_size (int): Batch size used in experiment\n",
    "        parameters_for_each_round (list): List of model parameters for each round\n",
    "        hist (History): Flower History object containing metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    results_dict = {\n",
    "        'batch_size': batch_size,\n",
    "        'parameters_for_each_round': parameters_for_each_round,\n",
    "        'history': hist\n",
    "    }\n",
    "    \n",
    "    with open(save_file_name, 'wb') as f:  # Note: 'wb' for binary write mode\n",
    "        pickle.dump(results_dict, f)\n",
    "\n",
    "def load_experiment(file_name):\n",
    "    \"\"\"Load experiment results from a pickle file.\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Path to the results file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (batch_size, parameters_for_each_round, hist)\n",
    "    \"\"\"\n",
    "    with open(file_name, 'rb') as f:  # Note: 'rb' for binary read mode\n",
    "        results_dict = pickle.load(f)\n",
    "    \n",
    "    return (\n",
    "        results_dict['batch_size'],\n",
    "        results_dict['parameters_for_each_round'],\n",
    "        results_dict['history'],\n",
    "    )\n",
    "\n",
    "total_global_batch_results = []\n",
    "cs_bs_pairs = [(100, 8000)] # [(5, 20), (20, 50), (50, 200), (100, 250), (100, 1000), (100, 2000)]\n",
    "for cohort_size, batch_size in cs_bs_pairs:\n",
    "    global_batch_size = batch_size * cohort_size\n",
    "    train_cfg = federated_train_config.copy()\n",
    "    ratio = np.sqrt(cohort_size * batch_size / 1e6)\n",
    "    # ratio = 100 / 100 = 1 * 0.01\n",
    "    # if i multiply by batch size, i want to divide\n",
    "    print(\"----------------------------------------------------------\", ratio * 0.01)\n",
    "    train_cfg[\"client_learning_rate\"] = ratio * 0.01\n",
    "    #train_cfg[\"max_batches\"] = 1000\n",
    "\n",
    "    test_cfg = federated_test_config.copy()\n",
    "\n",
    "    parameters_for_each_round, hist = run_simulation(\n",
    "        num_rounds = 100,\n",
    "        num_total_clients = num_total_clients,\n",
    "        num_clients_per_round = cohort_size,\n",
    "        num_evaluate_clients = num_evaluate_clients,\n",
    "        min_available_clients = num_total_clients,\n",
    "        min_fit_clients = cohort_size,\n",
    "        min_evaluate_clients = num_evaluate_clients,\n",
    "        evaluate_fn = federated_evaluation_function,\n",
    "        on_fit_config_fn = lambda _: train_cfg,\n",
    "        on_evaluate_config_fn = lambda _: test_cfg,\n",
    "        initial_parameters = initial_parameters,\n",
    "        fit_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        evaluate_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        federated_client_generator = federated_client_generator,\n",
    "        server_learning_rate=server_learning_rate,\n",
    "        server_momentum=server_momentum,\n",
    "        accept_failures=accept_failures,\n",
    "        target_accuracy=0.60,\n",
    "        use_target_accuracy=True,\n",
    "        )\n",
    "\n",
    "    total_global_batch_results.append((global_batch_size, parameters_for_each_round, hist))\n",
    "    save_experiment(f\"results/federated_global_batch_results_{global_batch_size}.pkl\", global_batch_size, parameters_for_each_round=parameters_for_each_round, hist=hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_experiment(save_file_name, batch_size, parameters_for_each_round, hist):\n",
    "    \"\"\"Save experiment results using pickle.\n",
    "    \n",
    "    Args:\n",
    "        save_file_name (str): Path to save the results\n",
    "        batch_size (int): Batch size used in experiment\n",
    "        parameters_for_each_round (list): List of model parameters for each round\n",
    "        hist (History): Flower History object containing metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    results_dict = {\n",
    "        'batch_size': batch_size,\n",
    "        'parameters_for_each_round': parameters_for_each_round,\n",
    "        'history': hist\n",
    "    }\n",
    "    \n",
    "    with open(save_file_name, 'wb') as f:  # Note: 'wb' for binary write mode\n",
    "        pickle.dump(results_dict, f)\n",
    "\n",
    "def load_experiment(file_name):\n",
    "    \"\"\"Load experiment results from a pickle file.\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Path to the results file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (batch_size, parameters_for_each_round, hist)\n",
    "    \"\"\"\n",
    "    with open(file_name, 'rb') as f:  # Note: 'rb' for binary read mode\n",
    "        results_dict = pickle.load(f)\n",
    "    \n",
    "    return (\n",
    "        results_dict['batch_size'],\n",
    "        results_dict['parameters_for_each_round'],\n",
    "        results_dict['history'],\n",
    "    )\n",
    "\n",
    "total_global_batch_results = []\n",
    "cs_bs_pairs = [(100, 8000)] # [(5, 20), (20, 50), (50, 200), (100, 250), (100, 1000), (100, 2000)]\n",
    "for cohort_size, batch_size in cs_bs_pairs:\n",
    "    global_batch_size = batch_size * cohort_size\n",
    "    train_cfg = federated_train_config.copy()\n",
    "    ratio = np.sqrt(cohort_size * batch_size / 1e6)\n",
    "    # ratio = 100 / 100 = 1 * 0.01\n",
    "    # if i multiply by batch size, i want to divide\n",
    "    print(\"----------------------------------------------------------\", ratio * 0.01)\n",
    "    train_cfg[\"client_learning_rate\"] = ratio * 0.01\n",
    "    #train_cfg[\"max_batches\"] = 1000\n",
    "\n",
    "    test_cfg = federated_test_config.copy()\n",
    "\n",
    "    parameters_for_each_round, hist = run_simulation(\n",
    "        num_rounds = 100,\n",
    "        num_total_clients = num_total_clients,\n",
    "        num_clients_per_round = cohort_size,\n",
    "        num_evaluate_clients = num_evaluate_clients,\n",
    "        min_available_clients = num_total_clients,\n",
    "        min_fit_clients = cohort_size,\n",
    "        min_evaluate_clients = num_evaluate_clients,\n",
    "        evaluate_fn = federated_evaluation_function,\n",
    "        on_fit_config_fn = lambda _: train_cfg,\n",
    "        on_evaluate_config_fn = lambda _: test_cfg,\n",
    "        initial_parameters = initial_parameters,\n",
    "        fit_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        evaluate_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        federated_client_generator = federated_client_generator,\n",
    "        server_learning_rate=server_learning_rate,\n",
    "        server_momentum=server_momentum,\n",
    "        accept_failures=accept_failures,\n",
    "        target_accuracy=0.60,\n",
    "        use_target_accuracy=True,\n",
    "        )\n",
    "\n",
    "    total_global_batch_results.append((global_batch_size, parameters_for_each_round, hist))\n",
    "    save_experiment(f\"results/federated_global_batch_results_{global_batch_size}.pkl\", global_batch_size, parameters_for_each_round=parameters_for_each_round, hist=hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "# Bottom-left: Compute Budget vs Training Time\n",
    "for global_batch_size, params, hist in total_global_batch_results:\n",
    "    times = []\n",
    "    samples = []\n",
    "    num_rounds = len(hist.metrics_distributed_fit['samples_processed'])\n",
    "    print(num_rounds)\n",
    "    cumulative_time = num_rounds * time_per_round\n",
    "\n",
    "    for round_idx, round_metrics in hist.metrics_distributed_fit['samples_processed']:\n",
    "        round_samples = [s for _, s in round_metrics['all']]\n",
    "        samples.append(np.sum(round_samples))\n",
    "    \n",
    "    \n",
    "    total_samples = np.sum(samples)\n",
    "    axes[0].plot(cumulative_time, total_samples, marker='o', label=f\"Global batch size: {global_batch_size}\")\n",
    "\n",
    "axes[0].set_xlabel(\"Total Training Time (s)\")\n",
    "axes[0].set_ylabel(\"Compute Budget (Total Samples Processed)\")\n",
    "axes[0].set_title(\"Compute Budget vs. Total Training Time\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Bottom-right: Noise Scale Analysis\n",
    "for global_batch_size, params, hist in total_global_batch_results:\n",
    "    noise_scales = []\n",
    "    for round_idx, round_metrics in hist.metrics_distributed_fit['noise_scale']:\n",
    "        round_noise_scales = [ns for _, ns in round_metrics['all']]\n",
    "        noise_scale = np.mean(round_noise_scales)\n",
    "        noise_scales.append(noise_scale)\n",
    "    \n",
    "    avg_noise_scale = np.mean(noise_scales)\n",
    "    x_axis = global_batch_size / (avg_noise_scale + 1e-10)\n",
    "    y_axis = 1 / (1 + (avg_noise_scale / global_batch_size))\n",
    "    \n",
    "    axes[1].plot(x_axis, y_axis, marker='o', label=f\"Global batch size: {global_batch_size}\")\n",
    "\n",
    "axes[1].set_xlabel(\"Global Batch Size / Noise Scale\")\n",
    "axes[1].set_ylabel(fr\"${{\\epsilon_\\text{{B}}}} / {{\\epsilon_\\text{{max}}}}$\")\n",
    "axes[1].set_title(\"Predicted Training Speed\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flbs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
