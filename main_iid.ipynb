{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T02:37:55.825303Z",
     "iopub.status.busy": "2025-03-18T02:37:55.824746Z",
     "iopub.status.idle": "2025-03-18T02:37:55.841027Z",
     "shell.execute_reply": "2025-03-18T02:37:55.840491Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T02:37:55.842908Z",
     "iopub.status.busy": "2025-03-18T02:37:55.842711Z",
     "iopub.status.idle": "2025-03-18T02:38:01.510401Z",
     "shell.execute_reply": "2025-03-18T02:38:01.509624Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from logging import INFO, DEBUG\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.signal import medfilt\n",
    "from flwr.common import log, ndarrays_to_parameters\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from src.common.client_utils import (\n",
    "    load_femnist_dataset,\n",
    "    get_network_generator_cnn as get_network_generator,\n",
    "    get_device,\n",
    "    get_model_parameters,\n",
    "    aggregate_weighted_average,\n",
    ")\n",
    "\n",
    "\n",
    "from src.flwr_core import (\n",
    "    set_all_seeds,\n",
    "    get_paths,\n",
    "    decompress_dataset,\n",
    "    get_flower_client_generator,\n",
    "    sample_random_clients,\n",
    "    get_federated_evaluation_function,\n",
    "    create_iid_partition,\n",
    ")\n",
    "\n",
    "from src.estimate import (\n",
    "    compute_critical_batch,\n",
    ")\n",
    "\n",
    "from src.experiments_simulation import (\n",
    "    run_simulation,\n",
    "    centralized_experiment,\n",
    ")\n",
    "\n",
    "from src.utils import get_centralized_acc_from_hist\n",
    "\n",
    "PathType = Path | str | None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T02:38:01.513828Z",
     "iopub.status.busy": "2025-03-18T02:38:01.513074Z",
     "iopub.status.idle": "2025-03-18T02:38:01.562794Z",
     "shell.execute_reply": "2025-03-18T02:38:01.561991Z"
    }
   },
   "outputs": [],
   "source": [
    "set_all_seeds()\n",
    "\n",
    "PATHS = get_paths()\n",
    "\n",
    "HOME_DIR = PATHS[\"home_dir\"]\n",
    "DATASET_DIR = PATHS[\"dataset_dir\"]\n",
    "DATA_DIR = PATHS[\"data_dir\"]\n",
    "CENTRALIZED_PARTITION = PATHS[\"centralized_partition\"]\n",
    "CENTRALIZED_MAPPING = PATHS[\"centralized_mapping\"]\n",
    "FEDERATED_PARTITION = PATHS[\"federated_partition\"]\n",
    "FEDERATED_IID_PARTITION = PATHS[\"iid_partition\"]\n",
    "\n",
    "# extract dataset from tar.gz\n",
    "decompress_dataset(PATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T02:38:01.565507Z",
     "iopub.status.busy": "2025-03-18T02:38:01.565241Z",
     "iopub.status.idle": "2025-03-18T02:38:11.082842Z",
     "shell.execute_reply": "2025-03-18T02:38:11.081803Z"
    }
   },
   "outputs": [],
   "source": [
    "max_clients = 1000\n",
    "create_iid_partition(PATHS, num_clients=max_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T02:38:11.087516Z",
     "iopub.status.busy": "2025-03-18T02:38:11.086105Z",
     "iopub.status.idle": "2025-03-18T02:38:11.136010Z",
     "shell.execute_reply": "2025-03-18T02:38:11.135206Z"
    }
   },
   "outputs": [],
   "source": [
    "NETWORK_GENERATOR = get_network_generator()\n",
    "SEED_NET = NETWORK_GENERATOR()\n",
    "SEED_MODEL_PARAMS = get_model_parameters(SEED_NET)\n",
    "CID_CLIENT_GENERATOR = get_flower_client_generator(NETWORK_GENERATOR, FEDERATED_IID_PARTITION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T02:38:11.138757Z",
     "iopub.status.busy": "2025-03-18T02:38:11.138464Z",
     "iopub.status.idle": "2025-03-18T02:38:11.766318Z",
     "shell.execute_reply": "2025-03-18T02:38:11.765804Z"
    }
   },
   "outputs": [],
   "source": [
    "# FL experiments\n",
    "experiment_batch_sizes = [32, 64, 128, 256, 512]\n",
    "cohort_sizes = [5, 10, 20, 50, 75, 100, 150]\n",
    "\n",
    "\n",
    "# Federated configuration dictionary\n",
    "federated_train_config = {\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 32,\n",
    "    \"client_learning_rate\": 0.01,\n",
    "    \"weight_decay\": 0,\n",
    "    \"num_workers\": 0,\n",
    "    \"max_batches\": 100,\n",
    "    \"return_params\": False\n",
    "\n",
    "}\n",
    "\n",
    "federated_test_config: dict[str, Any] = {\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 0,\n",
    "    \"max_batches\": 100,\n",
    "}\n",
    "\n",
    "num_rounds = 10\n",
    "num_total_clients = 100\n",
    "num_evaluate_clients = 0\n",
    "num_clients_per_round = 10\n",
    "\n",
    "initial_parameters = ndarrays_to_parameters(SEED_MODEL_PARAMS)\n",
    "\n",
    "federated_evaluation_function = get_federated_evaluation_function(\n",
    "    batch_size=federated_test_config[\"batch_size\"],\n",
    "    num_workers=federated_test_config[\"num_workers\"],\n",
    "    model_generator=NETWORK_GENERATOR,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    max_batches=None if \"max_batches\" not in federated_test_config else federated_test_config[\"max_batches\"],\n",
    ")\n",
    "\n",
    "server_learning_rate = 1.0\n",
    "server_momentum = 0.0\n",
    "accept_failures = False\n",
    "\n",
    "\n",
    "CID_CLIENT_GENERATOR = get_flower_client_generator(NETWORK_GENERATOR, FEDERATED_IID_PARTITION)\n",
    "\n",
    "list_of_ids = sample_random_clients(\n",
    "    num_total_clients, federated_train_config[\"batch_size\"],\n",
    "    CID_CLIENT_GENERATOR, max_clients=max_clients,\n",
    ")\n",
    "\n",
    "federated_client_generator = (\n",
    "    get_flower_client_generator(\n",
    "        NETWORK_GENERATOR, FEDERATED_IID_PARTITION, lambda seq_id: list_of_ids[seq_id]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating critical batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metric_keys = ['training_time', 'samples_processed', 'noise_scale', 'train_loss', 'actual_batches']\n",
    "import gc\n",
    "\n",
    "B_simples = []\n",
    "results = []\n",
    "batch_sizes = [16, 32, 64, 128, 256]\n",
    "for batch_size in batch_sizes:\n",
    "    train_cfg = federated_train_config.copy()\n",
    "    train_cfg[\"batch_size\"] = batch_size\n",
    "    ratio = np.sqrt(batch_size / 256)\n",
    "    train_cfg[\"client_learning_rate\"] = ratio * 0.01 # Same as centralized, but should be lower for FL\n",
    "    train_cfg[\"return_params\"] = True # needed to get g locals for estimation\n",
    "\n",
    "    test_cfg = federated_test_config.copy()\n",
    "    test_cfg[\"batch_size\"] = batch_size\n",
    "\n",
    "    \n",
    "    parameters_for_each_round, hist = run_simulation(\n",
    "        num_rounds = 10,\n",
    "        num_total_clients = num_total_clients,\n",
    "        num_clients_per_round = num_clients_per_round,\n",
    "        num_evaluate_clients = num_evaluate_clients,\n",
    "        min_available_clients = num_total_clients,\n",
    "        min_fit_clients = num_clients_per_round,\n",
    "        min_evaluate_clients = num_evaluate_clients,\n",
    "        evaluate_fn = federated_evaluation_function,\n",
    "        on_fit_config_fn = lambda _: train_cfg,\n",
    "        on_evaluate_config_fn = lambda _: test_cfg,\n",
    "        initial_parameters = initial_parameters,\n",
    "        fit_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        evaluate_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        federated_client_generator = federated_client_generator,\n",
    "        server_learning_rate=server_learning_rate,\n",
    "        server_momentum=server_momentum,\n",
    "        accept_failures=accept_failures,\n",
    "        target_accuracy=0.60,\n",
    "        use_target_accuracy=True\n",
    "        )\n",
    "    n_params = len(hist.metrics_distributed_fit.keys()) - 5\n",
    "    param_keys = list(set(hist.metrics_distributed_fit.keys()) - set(metric_keys))\n",
    "    hist_metrics = {key: hist.metrics_distributed_fit[key] for key in metric_keys}\n",
    "    params = [hist.metrics_distributed_fit[key] for key in param_keys]\n",
    "    del hist\n",
    "    #gc.collect()\n",
    "\n",
    "    res = (batch_size, parameters_for_each_round, hist_metrics, params)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "B_simples = [] # 244230\n",
    "n_clients = 10\n",
    "\n",
    "K = n_clients\n",
    "alpha = 0.9\n",
    "\n",
    "for k, res in enumerate(results):\n",
    "    batch_size, parameters_for_each_round, hist_metrics, params = res\n",
    "    B_small = batch_size\n",
    "    B_big = num_clients_per_round * batch_size\n",
    "    G_local = params\n",
    "    n_rounds = len(params[0])\n",
    "    params_filt = [params[i] for i in range(len(params)) if len(params[i]) == n_rounds]\n",
    "    G_local_by_rounds = [[params_filt[i][j][1]['all'] for i in range(len(params_filt))] for j in range(n_rounds)]\n",
    "    B_simples.append([0] * n_rounds)\n",
    "    for round_idx, G_local in enumerate(G_local_by_rounds):\n",
    "        G_local = [[el[1] for el in G_loc] for G_loc in G_local]\n",
    "        G_local_filt = [G_local[i] for i in range(len(G_local)) if len(G_local[i]) == 10]\n",
    "        G_local_filt = np.array(G_local_filt)\n",
    "        G_local_filt = G_local_filt.reshape(K, -1)\n",
    "\n",
    "        G_local_filt = [torch.tensor(G_local) for G_local in G_local_filt]\n",
    "        \n",
    "        local_norm_squared = torch.tensor([torch.norm(G_local)**2 for G_local in G_local_filt])\n",
    "\n",
    "        GBsmall_squared = local_norm_squared.sum() / K\n",
    "\n",
    "        G_big = sum(G_local_filt) / K\n",
    "\n",
    "        GBbig_squared = torch.norm(G_big)**2 \n",
    "\n",
    "        G2 = (1 / (B_big - B_small)) * (B_big * GBbig_squared - B_small * GBsmall_squared) \n",
    "\n",
    "        S = (B_small * B_big / (B_big - B_small)) * (GBbig_squared - GBsmall_squared)\n",
    "\n",
    "        B_simple = S/G2\n",
    "\n",
    "        B_simples[k][round_idx] = B_simple\n",
    "\n",
    "B_simples = [[el.item() for el in B_simple] for B_simple in B_simples]\n",
    "with open(\"B_simples.txt\", \"w\") as f:\n",
    "    f.write(str(B_simples))\n",
    "print(B_simples)\n",
    "\n",
    "# [[7331.360775759389, 10162.621287155156], [289988.8675930225, -76584.42877282941], [65100.4030868133, 15098.130444262237], [-52009.415823073214, -18557.47927118022, -11092165.511967614, -93741.82440415821], [-63007.125717336494, -74530.95902848525, -1084232.788576654, 15547940.564414758, 407759.4055155981, 272396.49553753383, -990024.0722842965, -127974.54799143146]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T02:38:11.769860Z",
     "iopub.status.busy": "2025-03-18T02:38:11.769191Z",
     "iopub.status.idle": "2025-03-18T02:38:11.823622Z",
     "shell.execute_reply": "2025-03-18T02:38:11.822994Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "metric_keys = ['training_time', 'samples_processed', 'noise_scale', 'train_loss', 'actual_batches']\n",
    "import gc\n",
    "\n",
    "B_simples = []\n",
    "results = []\n",
    "batch_sizes = [16, 32, 64, 128, 256]\n",
    "batch_times = []\n",
    "for batch_size in batch_sizes:\n",
    "    train_cfg = federated_train_config.copy()\n",
    "    train_cfg[\"batch_size\"] = batch_size\n",
    "    ratio = np.sqrt(batch_size / 256)\n",
    "    train_cfg[\"client_learning_rate\"] = ratio * 0.01 # Same as centralized, but should be lower for FL\n",
    "\n",
    "    test_cfg = federated_test_config.copy()\n",
    "    test_cfg[\"batch_size\"] = batch_size\n",
    "\n",
    "    local_list_of_ids = sample_random_clients(num_total_clients, train_cfg[\"batch_size\"], CID_CLIENT_GENERATOR, max_clients=max_clients)\n",
    "    local_federated_client_generator = get_flower_client_generator(NETWORK_GENERATOR, FEDERATED_IID_PARTITION, lambda seq_id: local_list_of_ids[seq_id])\n",
    "\n",
    "    parameters_for_each_round, hist = run_simulation(\n",
    "        num_rounds = 2,\n",
    "        num_total_clients = num_total_clients,\n",
    "        num_clients_per_round = num_clients_per_round,\n",
    "        num_evaluate_clients = num_evaluate_clients,\n",
    "        min_available_clients = num_total_clients,\n",
    "        min_fit_clients = num_clients_per_round,\n",
    "        min_evaluate_clients = num_evaluate_clients,\n",
    "        evaluate_fn = federated_evaluation_function,\n",
    "        on_fit_config_fn = lambda _: train_cfg,\n",
    "        on_evaluate_config_fn = lambda _: test_cfg,\n",
    "        initial_parameters = initial_parameters,\n",
    "        fit_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        evaluate_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        federated_client_generator = local_federated_client_generator,\n",
    "        server_learning_rate=server_learning_rate,\n",
    "        server_momentum=server_momentum,\n",
    "        accept_failures=accept_failures,\n",
    "        )\n",
    "\n",
    "    times = []\n",
    "    for round_idx, round_metrics in hist.metrics_distributed_fit['training_time']:\n",
    "        if round_idx > num_rounds:\n",
    "            break\n",
    "        round_times = [t for _, t in round_metrics['all']]\n",
    "        times.append(np.mean(round_times))\n",
    "\n",
    "    cumulative_time = np.sum(times)\n",
    "    batch_times.append((batch_size, cumulative_time, times))\n",
    "    #n_params = len(hist.metrics_distributed_fit.keys()) - 5\n",
    "    #param_keys = list(set(hist.metrics_distributed_fit.keys()) - set(metric_keys))\n",
    "    #hist_metrics = {key: hist.metrics_distributed_fit[key] for key in metric_keys}\n",
    "    #params = [hist.metrics_distributed_fit[key] for key in param_keys]\n",
    "    #del hist\n",
    "    #gc.collect()\n",
    "\n",
    "    #res = (batch_size, parameters_for_each_round, hist_metrics, params)\n",
    "    #results.append(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_simples = [[-18407.686925566362, -13762.505288528548, -29665.108319094634, -12927.35371816805, 4372.760959719874, 10805.953609141283, -21040.561981702886, 23238.6966912209, -20496.066521615783, 40973.22031090615], [-40754.83941407597, -94828.01133622219, -168606.6061223791, -13053.941180392021, -12196.393251686653, 139131.94393159807, -18836.993638551605, 14911.932928595123, 71887.69261004392, -429673.47555676906], [-50326.972540666844, -221078.50863027747, -12331.574093571728, -10559.19947656049, -533488.2914706912, 72097.91998244265, 2756775.10465039, -37545.04286075924, -215591.0786897387, -124943.4867790073], [-86654.8377399388, 70070.96612662714, 869667.1183916295, 62085.1567899368, 47205.20980176431, -64801.69123063742, 59619.394715874994, 43996.21136494777, 76026.20309461898, 370612.13195318024], [-37359.721294111616, -179548.41406089318, -266947.3063659719, -956503.1802564623, 272930.986020992, -264428.3445225671, 100874.68615423376, -94089.052502197, -138475.6462939103, -258445.3923542993]]\n",
    "\n",
    "batch_sizes = [16, 32, 64, 128, 256]\n",
    "B_simples = [[abs(el) for el in subl] for subl in B_simples]\n",
    "# apply median filter for each sublist\n",
    "B_simple_median = [medfilt(subl, 5) for subl in B_simples]\n",
    "# plot nicely B_simple through the rounds\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for bs, B_sim in zip(batch_sizes, B_simple_median):\n",
    "    ax.plot(B_sim, label=f\"Batch size: {bs}\")\n",
    "ax.set_xlabel('Round')\n",
    "ax.set_ylabel('Batch Size')\n",
    "ax.set_title('Critical Batch Size through Rounds')\n",
    "ax.legend()\n",
    "# log scale y\n",
    "ax.set_yscale('log')\n",
    "#ax.set_ylim(0, 5000)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T02:38:11.827078Z",
     "iopub.status.busy": "2025-03-18T02:38:11.826738Z",
     "iopub.status.idle": "2025-03-18T02:38:11.863983Z",
     "shell.execute_reply": "2025-03-18T02:38:11.863350Z"
    }
   },
   "outputs": [],
   "source": [
    "#for b, cumulative_round_times, total_times in batch_times:\n",
    "#    print(\"Batch size: \", b)\n",
    "#    print(\"Total time: \", cumulative_round_times)\n",
    "#    print(\"Times per round: \", total_times)\n",
    "\"\"\"\n",
    "\n",
    "Batch size:  16\n",
    "Total time:  22.73178415029806\n",
    "Times per round:  [22.73178415029806]\n",
    "Batch size:  32\n",
    "Total time:  13.338960419098475\n",
    "Times per round:  [13.338960419098475]\n",
    "Batch size:  64\n",
    "Total time:  5.612612966999677\n",
    "Times per round:  [5.612612966999677]\n",
    "Batch size:  128\n",
    "Total time:  3.1369846047005012\n",
    "Times per round:  [3.1369846047005012]\n",
    "Batch size:  256\n",
    "Total time:  1.3977464394006347\n",
    "Times per round:  [1.3977464394006347]\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T02:38:11.870391Z",
     "iopub.status.busy": "2025-03-18T02:38:11.870150Z",
     "iopub.status.idle": "2025-03-18T02:52:03.869680Z",
     "shell.execute_reply": "2025-03-18T02:52:03.868984Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_experiment(save_file_name, batch_size, parameters_for_each_round, hist):\n",
    "    \"\"\"Save experiment results using pickle.\n",
    "    \n",
    "    Args:\n",
    "        save_file_name (str): Path to save the results\n",
    "        batch_size (int): Batch size used in experiment\n",
    "        parameters_for_each_round (list): List of model parameters for each round\n",
    "        hist (History): Flower History object containing metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    results_dict = {\n",
    "        'batch_size': batch_size,\n",
    "        'parameters_for_each_round': parameters_for_each_round,\n",
    "        'history': hist\n",
    "    }\n",
    "    \n",
    "    with open(save_file_name, 'wb') as f:  # Note: 'wb' for binary write mode\n",
    "        pickle.dump(results_dict, f)\n",
    "\n",
    "def load_experiment(file_name):\n",
    "    \"\"\"Load experiment results from a pickle file.\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Path to the results file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (batch_size, parameters_for_each_round, hist)\n",
    "    \"\"\"\n",
    "    with open(file_name, 'rb') as f:  # Note: 'rb' for binary read mode\n",
    "        results_dict = pickle.load(f)\n",
    "    \n",
    "    return (\n",
    "        results_dict['batch_size'],\n",
    "        results_dict['parameters_for_each_round'],\n",
    "        results_dict['history'],\n",
    "    )\n",
    "\n",
    "total_batch_results = []\n",
    "\n",
    "experiment_batch_sizes = [256, 128, 64, 32, 16]\n",
    "for batch_size in experiment_batch_sizes:\n",
    "    print(f\"------------------------------------------------- BATCH SIZE: {batch_size} ---------------------------------------------------------------\")\n",
    "    train_cfg = federated_train_config.copy()\n",
    "    train_cfg[\"batch_size\"] = batch_size\n",
    "    ratio = np.sqrt(batch_size / 256) # non-iid ratio, 32 not working, 64 not working, 128 not working, \n",
    "    learning_rate = ratio * 0.01 # Same as centralized, but should be lower for FL\n",
    "    train_cfg[\"client_learning_rate\"] = learning_rate\n",
    "\n",
    "    test_cfg = federated_test_config.copy()\n",
    "    test_cfg[\"batch_size\"] = batch_size\n",
    "\n",
    "    local_list_of_ids = sample_random_clients(num_total_clients, train_cfg[\"batch_size\"], CID_CLIENT_GENERATOR, max_clients=max_clients)\n",
    "    local_federated_client_generator = get_flower_client_generator(NETWORK_GENERATOR, FEDERATED_IID_PARTITION, lambda seq_id: local_list_of_ids[seq_id])\n",
    "\n",
    "    parameters_for_each_round, hist = run_simulation(\n",
    "        num_rounds = num_rounds,\n",
    "        num_total_clients = num_total_clients,\n",
    "        num_clients_per_round = num_clients_per_round,\n",
    "        num_evaluate_clients = num_evaluate_clients,\n",
    "        min_available_clients = num_total_clients,\n",
    "        min_fit_clients = num_clients_per_round,\n",
    "        min_evaluate_clients = num_evaluate_clients,\n",
    "        evaluate_fn = federated_evaluation_function,\n",
    "        on_fit_config_fn = lambda _: train_cfg,\n",
    "        on_evaluate_config_fn = lambda _: test_cfg,\n",
    "        initial_parameters = initial_parameters,\n",
    "        fit_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        evaluate_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        federated_client_generator = local_federated_client_generator,\n",
    "        server_learning_rate=server_learning_rate,\n",
    "        server_momentum=server_momentum,\n",
    "        accept_failures=accept_failures,\n",
    "        target_accuracy=0.60,\n",
    "        use_target_accuracy=True,\n",
    "        )\n",
    "\n",
    "    total_batch_results.append((batch_size, parameters_for_each_round, hist))\n",
    "    save_experiment(fr\"results/IID_federated_local_batch_results_{batch_size}.pkl\", batch_size, parameters_for_each_round, hist)\n",
    "    # open a file, and append to it batch size, learnign rate, number of rounds taken to reach target accuracy\n",
    "    with open(\"bruh_file.txt\", \"a\") as f:\n",
    "        f.write(f\"Batch size: {batch_size}, Learning rate: {learning_rate}, Number of rounds taken to reach target accuracy: {len(hist.metrics_centralized['accuracy'])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T02:52:03.872263Z",
     "iopub.status.busy": "2025-03-18T02:52:03.872085Z",
     "iopub.status.idle": "2025-03-18T03:51:24.346511Z",
     "shell.execute_reply": "2025-03-18T03:51:24.345634Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_experiment(save_file_name, batch_size, parameters_for_each_round, hist):\n",
    "    \"\"\"Save experiment results using pickle.\n",
    "    \n",
    "    Args:\n",
    "        save_file_name (str): Path to save the results\n",
    "        batch_size (int): Batch size used in experiment\n",
    "        parameters_for_each_round (list): List of model parameters for each round\n",
    "        hist (History): Flower History object containing metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    results_dict = {\n",
    "        'batch_size': batch_size,\n",
    "        'parameters_for_each_round': parameters_for_each_round,\n",
    "        'history': hist\n",
    "    }\n",
    "    \n",
    "    with open(save_file_name, 'wb') as f:  # Note: 'wb' for binary write mode\n",
    "        pickle.dump(results_dict, f)\n",
    "\n",
    "def load_experiment(file_name):\n",
    "    \"\"\"Load experiment results from a pickle file.\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Path to the results file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (batch_size, parameters_for_each_round, hist)\n",
    "    \"\"\"\n",
    "    with open(file_name, 'rb') as f:  # Note: 'rb' for binary read mode\n",
    "        results_dict = pickle.load(f)\n",
    "    \n",
    "    return (\n",
    "        results_dict['batch_size'],\n",
    "        results_dict['parameters_for_each_round'],\n",
    "        results_dict['history'],\n",
    "    )\n",
    "\n",
    "total_cohort_results = []\n",
    "cohort_sizes =  [5, 10, 20, 50, 75, 100]\n",
    "for cohort_size in cohort_sizes:\n",
    "    train_cfg = federated_train_config.copy()\n",
    "    ratio = np.sqrt(cohort_size / 100)\n",
    "    train_cfg[\"client_learning_rate\"] = ratio * 0.01\n",
    "\n",
    "    test_cfg = federated_test_config.copy()\n",
    "\n",
    "    parameters_for_each_round, hist = run_simulation(\n",
    "        num_rounds = 10,\n",
    "        num_total_clients = num_total_clients,\n",
    "        num_clients_per_round = cohort_size,\n",
    "        num_evaluate_clients = num_evaluate_clients,\n",
    "        min_available_clients = num_total_clients,\n",
    "        min_fit_clients = cohort_size,\n",
    "        min_evaluate_clients = num_evaluate_clients,\n",
    "        evaluate_fn = federated_evaluation_function,\n",
    "        on_fit_config_fn = lambda _: train_cfg,\n",
    "        on_evaluate_config_fn = lambda _: test_cfg,\n",
    "        initial_parameters = initial_parameters,\n",
    "        fit_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        evaluate_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        federated_client_generator = federated_client_generator,\n",
    "        server_learning_rate=server_learning_rate,\n",
    "        server_momentum=server_momentum,\n",
    "        accept_failures=accept_failures,\n",
    "        target_accuracy=0.60,\n",
    "        use_target_accuracy=True,\n",
    "        )\n",
    "\n",
    "    total_cohort_results.append((cohort_size, parameters_for_each_round, hist))\n",
    "    save_experiment(f\"results/IID_federated_cohort_results_{cohort_size}.pkl\", cohort_size, parameters_for_each_round=parameters_for_each_round, hist=hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T03:51:24.353138Z",
     "iopub.status.busy": "2025-03-18T03:51:24.352847Z",
     "iopub.status.idle": "2025-03-18T06:20:03.207383Z",
     "shell.execute_reply": "2025-03-18T06:20:03.202826Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_experiment(save_file_name, batch_size, parameters_for_each_round, hist):\n",
    "    \"\"\"Save experiment results using pickle.\n",
    "    \n",
    "    Args:\n",
    "        save_file_name (str): Path to save the results\n",
    "        batch_size (int): Batch size used in experiment\n",
    "        parameters_for_each_round (list): List of model parameters for each round\n",
    "        hist (History): Flower History object containing metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    results_dict = {\n",
    "        'batch_size': batch_size,\n",
    "        'parameters_for_each_round': parameters_for_each_round,\n",
    "        'history': hist\n",
    "    }\n",
    "    \n",
    "    with open(save_file_name, 'wb') as f:  # Note: 'wb' for binary write mode\n",
    "        pickle.dump(results_dict, f)\n",
    "\n",
    "def load_experiment(file_name):\n",
    "    \"\"\"Load experiment results from a pickle file.\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Path to the results file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (batch_size, parameters_for_each_round, hist)\n",
    "    \"\"\"\n",
    "    with open(file_name, 'rb') as f:  # Note: 'rb' for binary read mode\n",
    "        results_dict = pickle.load(f)\n",
    "    \n",
    "    return (\n",
    "        results_dict['batch_size'],\n",
    "        results_dict['parameters_for_each_round'],\n",
    "        results_dict['history'],\n",
    "    )\n",
    "\n",
    "total_global_batch_results = []\n",
    "cs_bs_pairs = [(100, 4000), (100, 12000)]#[(5, 20), (20, 50), (50, 200), (100, 250), (100, 1000), (100, 2000), (100, 4000), (100, 12000)]\n",
    "for cohort_size, batch_size in cs_bs_pairs:\n",
    "    global_batch_size = batch_size * cohort_size\n",
    "    train_cfg = federated_train_config.copy()\n",
    "    ratio = np.sqrt(cohort_size * batch_size / 1e6)\n",
    "    # if i multiply by batch size, i want to divide\n",
    "    train_cfg[\"client_learning_rate\"] = ratio * 0.01\n",
    "    #train_cfg[\"max_batches\"] = 1000\n",
    "\n",
    "    test_cfg = federated_test_config.copy()\n",
    "\n",
    "    parameters_for_each_round, hist = run_simulation(\n",
    "        num_rounds = 10,\n",
    "        num_total_clients = num_total_clients,\n",
    "        num_clients_per_round = cohort_size,\n",
    "        num_evaluate_clients = num_evaluate_clients,\n",
    "        min_available_clients = num_total_clients,\n",
    "        min_fit_clients = cohort_size,\n",
    "        min_evaluate_clients = num_evaluate_clients,\n",
    "        evaluate_fn = federated_evaluation_function,\n",
    "        on_fit_config_fn = lambda _: train_cfg,\n",
    "        on_evaluate_config_fn = lambda _: test_cfg,\n",
    "        initial_parameters = initial_parameters,\n",
    "        fit_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        evaluate_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        federated_client_generator = federated_client_generator,\n",
    "        server_learning_rate=server_learning_rate,\n",
    "        server_momentum=server_momentum,\n",
    "        accept_failures=accept_failures,\n",
    "        target_accuracy=0.60,\n",
    "        use_target_accuracy=True,\n",
    "        )\n",
    "\n",
    "    total_global_batch_results.append((global_batch_size, parameters_for_each_round, hist))\n",
    "    save_experiment(f\"results/IID_federated_global_batch_results_{global_batch_size}.pkl\", global_batch_size, parameters_for_each_round=parameters_for_each_round, hist=hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flbs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
