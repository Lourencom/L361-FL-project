{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from logging import INFO, DEBUG\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.signal import medfilt\n",
    "from flwr.common import log, ndarrays_to_parameters\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from src.common.client_utils import (\n",
    "    load_femnist_dataset,\n",
    "    get_network_generator_cnn as get_network_generator,\n",
    "    get_device,\n",
    "    get_model_parameters,\n",
    "    aggregate_weighted_average,\n",
    ")\n",
    "\n",
    "\n",
    "from src.flwr_core import (\n",
    "    set_all_seeds,\n",
    "    get_paths,\n",
    "    decompress_dataset,\n",
    "    get_flower_client_generator,\n",
    "    sample_random_clients,\n",
    "    get_federated_evaluation_function,\n",
    "    create_iid_partition,\n",
    ")\n",
    "\n",
    "from src.estimate import (\n",
    "    compute_critical_batch,\n",
    ")\n",
    "\n",
    "from src.experiments_simulation import (\n",
    "    run_simulation,\n",
    "    centralized_experiment,\n",
    ")\n",
    "\n",
    "from src.utils import get_centralized_acc_from_hist\n",
    "\n",
    "PathType = Path | str | None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds()\n",
    "\n",
    "PATHS = get_paths()\n",
    "\n",
    "HOME_DIR = PATHS[\"home_dir\"]\n",
    "DATASET_DIR = PATHS[\"dataset_dir\"]\n",
    "DATA_DIR = PATHS[\"data_dir\"]\n",
    "CENTRALIZED_PARTITION = PATHS[\"centralized_partition\"]\n",
    "CENTRALIZED_MAPPING = PATHS[\"centralized_mapping\"]\n",
    "FEDERATED_PARTITION = PATHS[\"federated_partition\"]\n",
    "FEDERATED_IID_PARTITION = PATHS[\"iid_partition\"]\n",
    "\n",
    "# extract dataset from tar.gz\n",
    "decompress_dataset(PATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_iid_partition(PATHS, num_clients=3230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETWORK_GENERATOR = get_network_generator()\n",
    "SEED_NET = NETWORK_GENERATOR()\n",
    "SEED_MODEL_PARAMS = get_model_parameters(SEED_NET)\n",
    "CID_CLIENT_GENERATOR = get_flower_client_generator(NETWORK_GENERATOR, FEDERATED_IID_PARTITION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centralized experiments\n",
    "centralized_experiment_batch_sizes = [32, 64, 128, 256, 512, 1024]\n",
    "\n",
    "# Load the centralized dataset using the same function as in FL.\n",
    "# The centralized mapping folder should be the one used in the FL centralized experiment.\n",
    "centralized_train_dataset = load_femnist_dataset(data_dir=DATA_DIR,mapping=CENTRALIZED_MAPPING, name=\"train\")\n",
    "centralized_test_dataset = load_femnist_dataset(data_dir=DATA_DIR, mapping=CENTRALIZED_MAPPING, name=\"test\")\n",
    "\n",
    "centralized_train_config = {\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 32,\n",
    "    \"client_learning_rate\": 0.01,\n",
    "    \"weight_decay\": 0,\n",
    "    \"num_workers\": 0,\n",
    "    \"max_batches\": 100,\n",
    "}\n",
    "\n",
    "centralized_test_config = {\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 0,\n",
    "    \"max_batches\": 100,\n",
    "    \"target_accuracy\": 0.70,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FL experiments\n",
    "experiment_batch_sizes = [16, 32, 64, 128, 256]\n",
    "cohort_sizes = [5, 10, 20, 50, 75, 100]\n",
    "\n",
    "\n",
    "# Federated configuration dictionary\n",
    "federated_train_config = {\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 32,\n",
    "    \"client_learning_rate\": 0.01,\n",
    "    \"weight_decay\": 0,\n",
    "    \"num_workers\": 0,\n",
    "    \"max_batches\": 100,\n",
    "}\n",
    "\n",
    "federated_test_config: dict[str, Any] = {\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 0,\n",
    "    \"max_batches\": 100,\n",
    "}\n",
    "\n",
    "num_rounds = 10\n",
    "num_total_clients = 100\n",
    "num_evaluate_clients = 0\n",
    "num_clients_per_round = 10\n",
    "\n",
    "initial_parameters = ndarrays_to_parameters(SEED_MODEL_PARAMS)\n",
    "\n",
    "federated_evaluation_function = get_federated_evaluation_function(\n",
    "    batch_size=federated_test_config[\"batch_size\"],\n",
    "    num_workers=federated_test_config[\"num_workers\"],\n",
    "    model_generator=NETWORK_GENERATOR,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    max_batches=None if \"max_batches\" not in federated_test_config else federated_test_config[\"max_batches\"],\n",
    ")\n",
    "\n",
    "server_learning_rate = 1.0\n",
    "server_momentum = 0.0\n",
    "accept_failures = False\n",
    "\n",
    "\n",
    "CID_CLIENT_GENERATOR = get_flower_client_generator(NETWORK_GENERATOR, FEDERATED_IID_PARTITION)\n",
    "\n",
    "list_of_ids = sample_random_clients(\n",
    "    num_total_clients, federated_train_config[\"batch_size\"],\n",
    "    CID_CLIENT_GENERATOR,\n",
    ")\n",
    "\n",
    "federated_client_generator = (\n",
    "    get_flower_client_generator(\n",
    "        NETWORK_GENERATOR, FEDERATED_IID_PARTITION, lambda seq_id: list_of_ids[seq_id]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated varyig local batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_keys = ['training_time', 'samples_processed', 'noise_scale', 'train_loss', 'actual_batches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_simples = []\n",
    "results = []\n",
    "batch_sizes = [16, 32, 64, 128, 256]\n",
    "for batch_size in batch_sizes:\n",
    "    train_cfg = federated_train_config.copy()\n",
    "    train_cfg[\"batch_size\"] = batch_size\n",
    "    ratio = np.sqrt(batch_size / 256)\n",
    "    train_cfg[\"client_learning_rate\"] = ratio * 0.01 # Same as centralized, but should be lower for FL\n",
    "\n",
    "    test_cfg = federated_test_config.copy()\n",
    "    test_cfg[\"batch_size\"] = batch_size\n",
    "\n",
    "    local_list_of_ids = sample_random_clients(num_total_clients, train_cfg[\"batch_size\"], CID_CLIENT_GENERATOR)\n",
    "    local_federated_client_generator = get_flower_client_generator(NETWORK_GENERATOR, FEDERATED_PARTITION, lambda seq_id: local_list_of_ids[seq_id])\n",
    "\n",
    "    parameters_for_each_round, hist = run_simulation(\n",
    "        num_rounds = 10,\n",
    "        num_total_clients = num_total_clients,\n",
    "        num_clients_per_round = num_clients_per_round,\n",
    "        num_evaluate_clients = num_evaluate_clients,\n",
    "        min_available_clients = num_total_clients,\n",
    "        min_fit_clients = num_clients_per_round,\n",
    "        min_evaluate_clients = num_evaluate_clients,\n",
    "        evaluate_fn = federated_evaluation_function,\n",
    "        on_fit_config_fn = lambda _: train_cfg,\n",
    "        on_evaluate_config_fn = lambda _: test_cfg,\n",
    "        initial_parameters = initial_parameters,\n",
    "        fit_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        evaluate_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        federated_client_generator = local_federated_client_generator,\n",
    "        server_learning_rate=server_learning_rate,\n",
    "        server_momentum=server_momentum,\n",
    "        accept_failures=accept_failures,\n",
    "        target_accuracy=0.70,\n",
    "        use_target_accuracy=True\n",
    "        )\n",
    "    n_params = len(hist.metrics_distributed_fit.keys()) - 5\n",
    "    param_keys = list(set(hist.metrics_distributed_fit.keys()) - set(metric_keys))\n",
    "    hist_metrics = {key: hist.metrics_distributed_fit[key] for key in metric_keys}\n",
    "    params = [hist.metrics_distributed_fit[key] for key in param_keys]\n",
    "    del hist\n",
    "    #gc.collect()\n",
    "\n",
    "    res = (batch_size, parameters_for_each_round, hist_metrics, params)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "B_simples = [] # 244230\n",
    "n_clients = 10\n",
    "\n",
    "K = n_clients\n",
    "alpha = 0.9\n",
    "\n",
    "for k, res in enumerate(results):\n",
    "    batch_size, parameters_for_each_round, hist_metrics, params = res\n",
    "    B_small = batch_size\n",
    "    B_big = num_clients_per_round * batch_size\n",
    "    G_local = params\n",
    "    n_rounds = len(params[0])\n",
    "    params_filt = [params[i] for i in range(len(params)) if len(params[i]) == n_rounds]\n",
    "    G_local_by_rounds = [[params_filt[i][j][1]['all'] for i in range(len(params_filt))] for j in range(n_rounds)]\n",
    "    B_simples.append([0] * n_rounds)\n",
    "    for round_idx, G_local in enumerate(G_local_by_rounds):\n",
    "        G_local = [[el[1] for el in G_loc] for G_loc in G_local]\n",
    "        G_local_filt = [G_local[i] for i in range(len(G_local)) if len(G_local[i]) == 10]\n",
    "        G_local_filt = np.array(G_local_filt)\n",
    "        G_local_filt = G_local_filt.reshape(K, -1)\n",
    "\n",
    "        G_local_filt = [torch.tensor(G_local) for G_local in G_local_filt]\n",
    "        \n",
    "        local_norm_squared = torch.tensor([torch.norm(G_local)**2 for G_local in G_local_filt])\n",
    "\n",
    "        GBsmall_squared = local_norm_squared.sum() / K\n",
    "\n",
    "        G_big = sum(G_local_filt) / K\n",
    "\n",
    "        GBbig_squared = torch.norm(G_big)**2 \n",
    "\n",
    "        G2 = (1 / (B_big - B_small)) * (B_big * GBbig_squared - B_small * GBsmall_squared) \n",
    "\n",
    "        S = (B_small * B_big / (B_big - B_small)) * (GBbig_squared - GBsmall_squared)\n",
    "\n",
    "        B_simple = S/G2\n",
    "\n",
    "        B_simples[k][round_idx] = B_simple\n",
    "\n",
    "B_simples = [[el.item() for el in B_simple] for B_simple in B_simples]\n",
    "print(B_simples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_simples = [[abs(el) for el in subl] for subl in B_simples]\n",
    "# apply median filter for each sublist\n",
    "B_simple_median = [medfilt(subl, 5) for subl in B_simples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot nicely B_simple through the rounds\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for bs, B_sim in zip(batch_sizes, B_simple_median):\n",
    "    ax.plot(B_sim, label=f\"Batch size: {bs}\")\n",
    "ax.set_xlabel('Round')\n",
    "ax.set_ylabel('Batch Size')\n",
    "ax.set_title('Critical Batch Size through Rounds')\n",
    "ax.legend()\n",
    "# log scale y\n",
    "ax.set_yscale('log')\n",
    "#ax.set_ylim(0, 5000)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_experiment(save_file_name, batch_size, parameters_for_each_round, hist):\n",
    "    \"\"\"Save experiment results using pickle.\n",
    "    \n",
    "    Args:\n",
    "        save_file_name (str): Path to save the results\n",
    "        batch_size (int): Batch size used in experiment\n",
    "        parameters_for_each_round (list): List of model parameters for each round\n",
    "        hist (History): Flower History object containing metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    results_dict = {\n",
    "        'batch_size': batch_size,\n",
    "        'parameters_for_each_round': parameters_for_each_round,\n",
    "        'history': hist\n",
    "    }\n",
    "    \n",
    "    with open(save_file_name, 'wb') as f:  # Note: 'wb' for binary write mode\n",
    "        pickle.dump(results_dict, f)\n",
    "\n",
    "def load_experiment(file_name):\n",
    "    \"\"\"Load experiment results from a pickle file.\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Path to the results file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (batch_size, parameters_for_each_round, hist)\n",
    "    \"\"\"\n",
    "    with open(file_name, 'rb') as f:  # Note: 'rb' for binary read mode\n",
    "        results_dict = pickle.load(f)\n",
    "    \n",
    "    return (\n",
    "        results_dict['batch_size'],\n",
    "        results_dict['parameters_for_each_round'],\n",
    "        results_dict['history'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "varying local batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batch_results = []\n",
    "\n",
    "for batch_size in experiment_batch_sizes:\n",
    "    train_cfg = federated_train_config.copy()\n",
    "    train_cfg[\"batch_size\"] = batch_size\n",
    "    ratio = np.sqrt(batch_size / 256)\n",
    "    train_cfg[\"client_learning_rate\"] = ratio * 0.01 # Same as centralized, but should be lower for FL\n",
    "\n",
    "    test_cfg = federated_test_config.copy()\n",
    "    test_cfg[\"batch_size\"] = batch_size\n",
    "\n",
    "    local_list_of_ids = sample_random_clients(num_total_clients, train_cfg[\"batch_size\"], CID_CLIENT_GENERATOR)\n",
    "    local_federated_client_generator = get_flower_client_generator(NETWORK_GENERATOR, FEDERATED_PARTITION, lambda seq_id: local_list_of_ids[seq_id])\n",
    "\n",
    "    parameters_for_each_round, hist = run_simulation(\n",
    "        num_rounds = num_rounds,\n",
    "        num_total_clients = num_total_clients,\n",
    "        num_clients_per_round = num_clients_per_round,\n",
    "        num_evaluate_clients = num_evaluate_clients,\n",
    "        min_available_clients = num_total_clients,\n",
    "        min_fit_clients = num_clients_per_round,\n",
    "        min_evaluate_clients = num_evaluate_clients,\n",
    "        evaluate_fn = federated_evaluation_function,\n",
    "        on_fit_config_fn = lambda _: train_cfg,\n",
    "        on_evaluate_config_fn = lambda _: test_cfg,\n",
    "        initial_parameters = initial_parameters,\n",
    "        fit_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        evaluate_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        federated_client_generator = local_federated_client_generator,\n",
    "        server_learning_rate=server_learning_rate,\n",
    "        server_momentum=server_momentum,\n",
    "        accept_failures=accept_failures,\n",
    "        target_accuracy=0.70,\n",
    "        use_target_accuracy=True,\n",
    "        )\n",
    "\n",
    "    save_experiment(f\"iid_federated_batch_results_{batch_size}.pkl\", batch_size, parameters_for_each_round, hist)\n",
    "    total_batch_results.append((batch_size, parameters_for_each_round, hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "# Bottom-left: Compute Budget vs Training Time\n",
    "for batch_size, params, hist in total_batch_results:\n",
    "    times = []\n",
    "    samples = []\n",
    "    for round_idx, round_metrics in hist.metrics_distributed_fit['training_time']:\n",
    "        round_times = [t for _, t in round_metrics['all']]\n",
    "        times.append(np.mean(round_times))\n",
    "        \n",
    "    for round_idx, round_metrics in hist.metrics_distributed_fit['samples_processed']:\n",
    "        round_samples = [s for _, s in round_metrics['all']]\n",
    "        samples.append(np.sum(round_samples))\n",
    "    \n",
    "    cumulative_time = np.sum(times)\n",
    "    total_samples = np.sum(samples)\n",
    "    axes[0].plot(cumulative_time, total_samples, marker='o', label=f\"Local batch size: {batch_size}\")\n",
    "\n",
    "axes[0].set_xlabel(\"Total Training Time (s)\")\n",
    "axes[0].set_ylabel(\"Compute Budget (Total Samples Processed)\")\n",
    "axes[0].set_title(\"Compute Budget vs. Total Training Time\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Bottom-right: Noise Scale Analysis\n",
    "for batch_size, params, hist in total_batch_results:\n",
    "    noise_scales = []\n",
    "    for round_idx, round_metrics in hist.metrics_distributed_fit['noise_scale']:\n",
    "        round_noise_scales = [ns for _, ns in round_metrics['all']]\n",
    "        noise_scale = np.mean(round_noise_scales)\n",
    "        noise_scales.append(noise_scale)\n",
    "    \n",
    "    avg_noise_scale = np.mean(noise_scales)\n",
    "    x_axis = batch_size / (avg_noise_scale + 1e-10)\n",
    "    y_axis = 1 / (1 + (avg_noise_scale / batch_size))\n",
    "    \n",
    "    axes[1].plot(x_axis, y_axis, marker='o', label=f\"Batch size: {batch_size}\")\n",
    "\n",
    "axes[1].set_xlabel(\"Batch Size / Noise Scale\")\n",
    "axes[1].set_ylabel(fr\"${{\\epsilon_\\text{{B}}}} / {{\\epsilon_\\text{{max}}}}$\")\n",
    "axes[1].set_title(\"Predicted Training Speed\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "varying cohort size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_experiment(save_file_name, batch_size, parameters_for_each_round, hist):\n",
    "    \"\"\"Save experiment results using pickle.\n",
    "    \n",
    "    Args:\n",
    "        save_file_name (str): Path to save the results\n",
    "        batch_size (int): Batch size used in experiment\n",
    "        parameters_for_each_round (list): List of model parameters for each round\n",
    "        hist (History): Flower History object containing metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    results_dict = {\n",
    "        'batch_size': batch_size,\n",
    "        'parameters_for_each_round': parameters_for_each_round,\n",
    "        'history': hist\n",
    "    }\n",
    "    \n",
    "    with open(save_file_name, 'wb') as f:  # Note: 'wb' for binary write mode\n",
    "        pickle.dump(results_dict, f)\n",
    "\n",
    "def load_experiment(file_name):\n",
    "    \"\"\"Load experiment results from a pickle file.\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Path to the results file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (batch_size, parameters_for_each_round, hist)\n",
    "    \"\"\"\n",
    "    with open(file_name, 'rb') as f:  # Note: 'rb' for binary read mode\n",
    "        results_dict = pickle.load(f)\n",
    "    \n",
    "    return (\n",
    "        results_dict['batch_size'],\n",
    "        results_dict['parameters_for_each_round'],\n",
    "        results_dict['history'],\n",
    "    )\n",
    "\n",
    "total_cohort_results = []\n",
    "cohort_sizes =  [ 5, 10, 20, 50, 75, 100]\n",
    "for cohort_size in cohort_sizes:\n",
    "    train_cfg = federated_train_config.copy()\n",
    "    ratio = np.sqrt(cohort_size / 100)\n",
    "    train_cfg[\"client_learning_rate\"] = ratio * 0.01\n",
    "    #train_cfg[\"max_batches\"] = 1000\n",
    "\n",
    "    test_cfg = federated_test_config.copy()\n",
    "\n",
    "    parameters_for_each_round, hist = run_simulation(\n",
    "        num_rounds = 100,\n",
    "        num_total_clients = num_total_clients,\n",
    "        num_clients_per_round = cohort_size,\n",
    "        num_evaluate_clients = num_evaluate_clients,\n",
    "        min_available_clients = num_total_clients,\n",
    "        min_fit_clients = cohort_size,\n",
    "        min_evaluate_clients = num_evaluate_clients,\n",
    "        evaluate_fn = federated_evaluation_function,\n",
    "        on_fit_config_fn = lambda _: train_cfg,\n",
    "        on_evaluate_config_fn = lambda _: test_cfg,\n",
    "        initial_parameters = initial_parameters,\n",
    "        fit_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        evaluate_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        federated_client_generator = federated_client_generator,\n",
    "        server_learning_rate=server_learning_rate,\n",
    "        server_momentum=server_momentum,\n",
    "        accept_failures=accept_failures,\n",
    "        target_accuracy=0.70,\n",
    "        use_target_accuracy=True,\n",
    "        )\n",
    "\n",
    "    total_cohort_results.append((cohort_size, parameters_for_each_round, hist))\n",
    "    save_experiment(f\"results/iid_federated_cohort_results1_{cohort_size}.pkl\", cohort_size, parameters_for_each_round=parameters_for_each_round, hist=hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "time_per_round = 0.00427\n",
    "# Bottom-left: Compute Budget vs Training Time\n",
    "for cohort_size, params, hist in total_cohort_results:\n",
    "    times = []\n",
    "    samples = []\n",
    "    num_rounds = len(hist.metrics_distributed_fit['samples_processed'])\n",
    "    print(num_rounds)\n",
    "    cumulative_time = num_rounds * time_per_round\n",
    "\n",
    "    for round_idx, round_metrics in hist.metrics_distributed_fit['samples_processed']:\n",
    "        round_samples = [s for _, s in round_metrics['all']]\n",
    "        samples.append(np.sum(round_samples))\n",
    "    \n",
    "    \n",
    "    total_samples = np.sum(samples)\n",
    "    axes[0].plot(cumulative_time, total_samples, marker='o', label=f\"Cohort size: {cohort_size}\")\n",
    "\n",
    "axes[0].set_xlabel(\"Total Training Time (s)\")\n",
    "axes[0].set_ylabel(\"Compute Budget (Total Samples Processed)\")\n",
    "axes[0].set_title(\"Compute Budget vs. Total Training Time\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Bottom-right: Noise Scale Analysis\n",
    "for cohort_size, params, hist in total_cohort_results:\n",
    "    noise_scales = []\n",
    "    for round_idx, round_metrics in hist.metrics_distributed_fit['noise_scale']:\n",
    "        round_noise_scales = [ns for _, ns in round_metrics['all']]\n",
    "        noise_scale = np.mean(round_noise_scales)\n",
    "        noise_scales.append(noise_scale)\n",
    "    \n",
    "    avg_noise_scale = np.mean(noise_scales)\n",
    "    x_axis = cohort_size / (avg_noise_scale + 1e-10)\n",
    "    y_axis = 1 / (1 + (avg_noise_scale / cohort_size))\n",
    "    \n",
    "    axes[1].plot(x_axis, y_axis, marker='o', label=f\"Cohort size: {cohort_size}\")\n",
    "\n",
    "axes[1].set_xlabel(\"Cohort Size / Noise Scale\")\n",
    "axes[1].set_ylabel(fr\"${{\\epsilon_\\text{{B}}}} / {{\\epsilon_\\text{{max}}}}$\")\n",
    "axes[1].set_title(\"Predicted Training Speed\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "varying global batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_per_round = 0.00427\n",
    "times_per_round = [0.003793160191070807, 0.004481774148126263, 0.004956590107499679, 0.004278853300000662]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_experiment(save_file_name, batch_size, parameters_for_each_round, hist):\n",
    "    \"\"\"Save experiment results using pickle.\n",
    "    \n",
    "    Args:\n",
    "        save_file_name (str): Path to save the results\n",
    "        batch_size (int): Batch size used in experiment\n",
    "        parameters_for_each_round (list): List of model parameters for each round\n",
    "        hist (History): Flower History object containing metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    results_dict = {\n",
    "        'batch_size': batch_size,\n",
    "        'parameters_for_each_round': parameters_for_each_round,\n",
    "        'history': hist\n",
    "    }\n",
    "    \n",
    "    with open(save_file_name, 'wb') as f:  # Note: 'wb' for binary write mode\n",
    "        pickle.dump(results_dict, f)\n",
    "\n",
    "def load_experiment(file_name):\n",
    "    \"\"\"Load experiment results from a pickle file.\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Path to the results file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (batch_size, parameters_for_each_round, hist)\n",
    "    \"\"\"\n",
    "    with open(file_name, 'rb') as f:  # Note: 'rb' for binary read mode\n",
    "        results_dict = pickle.load(f)\n",
    "    \n",
    "    return (\n",
    "        results_dict['batch_size'],\n",
    "        results_dict['parameters_for_each_round'],\n",
    "        results_dict['history'],\n",
    "    )\n",
    "\n",
    "total_global_batch_results = []\n",
    "cs_bs_pairs = [(5, 20), (20, 50), (50, 200), (100, 250), (100, 1000), (100, 2000), (100, 12000)]\n",
    "for cohort_size, batch_size in cs_bs_pairs:\n",
    "    global_batch_size = batch_size * cohort_size\n",
    "    train_cfg = federated_train_config.copy()\n",
    "    ratio = np.sqrt(cohort_size * batch_size / 1e6)\n",
    "    # ratio = 100 / 100 = 1 * 0.01\n",
    "    # if i multiply by batch size, i want to divide\n",
    "    print(\"----------------------------------------------------------\", ratio * 0.01)\n",
    "    train_cfg[\"client_learning_rate\"] = ratio * 0.01\n",
    "    #train_cfg[\"max_batches\"] = 1000\n",
    "\n",
    "    test_cfg = federated_test_config.copy()\n",
    "\n",
    "    parameters_for_each_round, hist = run_simulation(\n",
    "        num_rounds = 100,\n",
    "        num_total_clients = num_total_clients,\n",
    "        num_clients_per_round = cohort_size,\n",
    "        num_evaluate_clients = num_evaluate_clients,\n",
    "        min_available_clients = num_total_clients,\n",
    "        min_fit_clients = cohort_size,\n",
    "        min_evaluate_clients = num_evaluate_clients,\n",
    "        evaluate_fn = federated_evaluation_function,\n",
    "        on_fit_config_fn = lambda _: train_cfg,\n",
    "        on_evaluate_config_fn = lambda _: test_cfg,\n",
    "        initial_parameters = initial_parameters,\n",
    "        fit_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        evaluate_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        federated_client_generator = federated_client_generator,\n",
    "        server_learning_rate=server_learning_rate,\n",
    "        server_momentum=server_momentum,\n",
    "        accept_failures=accept_failures,\n",
    "        target_accuracy=0.70,\n",
    "        use_target_accuracy=True,\n",
    "        )\n",
    "\n",
    "    total_global_batch_results.append((global_batch_size, parameters_for_each_round, hist))\n",
    "    save_experiment(f\"results/iid_federated_global_batch_results_{global_batch_size}.pkl\", global_batch_size, parameters_for_each_round=parameters_for_each_round, hist=hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_experiment(save_file_name, batch_size, parameters_for_each_round, hist):\n",
    "    \"\"\"Save experiment results using pickle.\n",
    "    \n",
    "    Args:\n",
    "        save_file_name (str): Path to save the results\n",
    "        batch_size (int): Batch size used in experiment\n",
    "        parameters_for_each_round (list): List of model parameters for each round\n",
    "        hist (History): Flower History object containing metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    results_dict = {\n",
    "        'batch_size': batch_size,\n",
    "        'parameters_for_each_round': parameters_for_each_round,\n",
    "        'history': hist\n",
    "    }\n",
    "    \n",
    "    with open(save_file_name, 'wb') as f:  # Note: 'wb' for binary write mode\n",
    "        pickle.dump(results_dict, f)\n",
    "\n",
    "def load_experiment(file_name):\n",
    "    \"\"\"Load experiment results from a pickle file.\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Path to the results file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (batch_size, parameters_for_each_round, hist)\n",
    "    \"\"\"\n",
    "    with open(file_name, 'rb') as f:  # Note: 'rb' for binary read mode\n",
    "        results_dict = pickle.load(f)\n",
    "    \n",
    "    return (\n",
    "        results_dict['batch_size'],\n",
    "        results_dict['parameters_for_each_round'],\n",
    "        results_dict['history'],\n",
    "    )\n",
    "\n",
    "total_global_batch_results = []\n",
    "cs_bs_pairs = [(100, 8000)] # [(5, 20), (20, 50), (50, 200), (100, 250), (100, 1000), (100, 2000)]\n",
    "for cohort_size, batch_size in cs_bs_pairs:\n",
    "    global_batch_size = batch_size * cohort_size\n",
    "    train_cfg = federated_train_config.copy()\n",
    "    ratio = np.sqrt(cohort_size * batch_size / 1e6)\n",
    "    # ratio = 100 / 100 = 1 * 0.01\n",
    "    # if i multiply by batch size, i want to divide\n",
    "    print(\"----------------------------------------------------------\", ratio * 0.01)\n",
    "    train_cfg[\"client_learning_rate\"] = ratio * 0.01\n",
    "    #train_cfg[\"max_batches\"] = 1000\n",
    "\n",
    "    test_cfg = federated_test_config.copy()\n",
    "\n",
    "    parameters_for_each_round, hist = run_simulation(\n",
    "        num_rounds = 100,\n",
    "        num_total_clients = num_total_clients,\n",
    "        num_clients_per_round = cohort_size,\n",
    "        num_evaluate_clients = num_evaluate_clients,\n",
    "        min_available_clients = num_total_clients,\n",
    "        min_fit_clients = cohort_size,\n",
    "        min_evaluate_clients = num_evaluate_clients,\n",
    "        evaluate_fn = federated_evaluation_function,\n",
    "        on_fit_config_fn = lambda _: train_cfg,\n",
    "        on_evaluate_config_fn = lambda _: test_cfg,\n",
    "        initial_parameters = initial_parameters,\n",
    "        fit_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        evaluate_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        federated_client_generator = federated_client_generator,\n",
    "        server_learning_rate=server_learning_rate,\n",
    "        server_momentum=server_momentum,\n",
    "        accept_failures=accept_failures,\n",
    "        target_accuracy=0.70,\n",
    "        use_target_accuracy=True,\n",
    "        )\n",
    "\n",
    "    total_global_batch_results.append((global_batch_size, parameters_for_each_round, hist))\n",
    "    save_experiment(f\"results/iid_federated_global_batch_results_{global_batch_size}.pkl\", global_batch_size, parameters_for_each_round=parameters_for_each_round, hist=hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "# Bottom-left: Compute Budget vs Training Time\n",
    "for global_batch_size, params, hist in total_global_batch_results:\n",
    "    times = []\n",
    "    samples = []\n",
    "    num_rounds = len(hist.metrics_distributed_fit['samples_processed'])\n",
    "    print(num_rounds)\n",
    "    cumulative_time = num_rounds * time_per_round\n",
    "\n",
    "    for round_idx, round_metrics in hist.metrics_distributed_fit['samples_processed']:\n",
    "        round_samples = [s for _, s in round_metrics['all']]\n",
    "        samples.append(np.sum(round_samples))\n",
    "    \n",
    "    \n",
    "    total_samples = np.sum(samples)\n",
    "    axes[0].plot(cumulative_time, total_samples, marker='o', label=f\"Global batch size: {global_batch_size}\")\n",
    "\n",
    "axes[0].set_xlabel(\"Total Training Time (s)\")\n",
    "axes[0].set_ylabel(\"Compute Budget (Total Samples Processed)\")\n",
    "axes[0].set_title(\"Compute Budget vs. Total Training Time\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Bottom-right: Noise Scale Analysis\n",
    "for global_batch_size, params, hist in total_global_batch_results:\n",
    "    noise_scales = []\n",
    "    for round_idx, round_metrics in hist.metrics_distributed_fit['noise_scale']:\n",
    "        round_noise_scales = [ns for _, ns in round_metrics['all']]\n",
    "        noise_scale = np.mean(round_noise_scales)\n",
    "        noise_scales.append(noise_scale)\n",
    "    \n",
    "    avg_noise_scale = np.mean(noise_scales)\n",
    "    x_axis = global_batch_size / (avg_noise_scale + 1e-10)\n",
    "    y_axis = 1 / (1 + (avg_noise_scale / global_batch_size))\n",
    "    \n",
    "    axes[1].plot(x_axis, y_axis, marker='o', label=f\"Global batch size: {global_batch_size}\")\n",
    "\n",
    "axes[1].set_xlabel(\"Global Batch Size / Noise Scale\")\n",
    "axes[1].set_ylabel(fr\"${{\\epsilon_\\text{{B}}}} / {{\\epsilon_\\text{{max}}}}$\")\n",
    "axes[1].set_title(\"Predicted Training Speed\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
