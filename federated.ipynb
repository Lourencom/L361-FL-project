{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "from typing import Any\n",
    "from logging import INFO, DEBUG\n",
    "from collections import defaultdict, OrderedDict\n",
    "from collections.abc import Sequence, Callable\n",
    "import numbers\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from enum import IntEnum\n",
    "import flwr\n",
    "from flwr.server import History, ServerConfig\n",
    "from flwr.server.strategy import FedAvgM as FedAvg, Strategy\n",
    "from flwr.common import log, NDArrays, Scalar, Parameters, ndarrays_to_parameters\n",
    "from flwr.client.client import Client\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from common.client_utils import (\n",
    "    Net,\n",
    "    load_femnist_dataset,\n",
    "    get_network_generator_cnn as get_network_generator,\n",
    "    train_femnist,\n",
    "    test_femnist,\n",
    "    save_history,\n",
    ")\n",
    "\n",
    "\n",
    "# Add new seeds here for easy autocomplete\n",
    "class Seeds(IntEnum):\n",
    "    \"\"\"Seeds for reproducibility.\"\"\"\n",
    "\n",
    "    DEFAULT = 1337\n",
    "\n",
    "\n",
    "np.random.seed(Seeds.DEFAULT)\n",
    "random.seed(Seeds.DEFAULT)\n",
    "torch.manual_seed(Seeds.DEFAULT)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "PathType = Path | str | None\n",
    "\n",
    "\n",
    "def get_device() -> str:\n",
    "    \"\"\"Get the device (cuda, mps, cpu).\"\"\"\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        device = \"mps\"\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = Path.cwd()\n",
    "dataset_dir: Path = home_dir / \"femnist\"\n",
    "data_dir: Path = dataset_dir / \"data\"\n",
    "centralized_partition: Path = dataset_dir / \"client_data_mappings\" / \"centralized\"\n",
    "centralized_mapping: Path = dataset_dir / \"client_data_mappings\" / \"centralized\" / \"0\"\n",
    "federated_partition: Path = dataset_dir / \"client_data_mappings\" / \"fed_natural\"\n",
    "\n",
    "# Decompress dataset\n",
    "if not dataset_dir.exists():\n",
    "    with tarfile.open(home_dir / \"femnist.tar.gz\", \"r:gz\") as tar:\n",
    "        tar.extractall(path=home_dir)\n",
    "    log(INFO, \"Dataset extracted in %s\", dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model_parameters(net: Module, parameters: NDArrays) -> Module:\n",
    "    \"\"\"Put a set of parameters into the model object.\"\"\"\n",
    "    weights = parameters\n",
    "    params_dict = zip(net.state_dict().keys(), weights, strict=False)\n",
    "    state_dict = OrderedDict({k: torch.from_numpy(np.copy(v)) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "    return net\n",
    "\n",
    "\n",
    "def get_model_parameters(net: Module) -> NDArrays:\n",
    "    \"\"\"Get the current model parameters as NDArrays.\"\"\"\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_noise_scale_from_gradients(grad_list, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Compute the noise scale (Bsimple) from a list of gradient vectors.\n",
    "    \n",
    "    Parameters:\n",
    "        grad_list (list[Tensor]): List of gradient vectors.\n",
    "        eps (float): Small constant for numerical stability.\n",
    "    \n",
    "    Returns:\n",
    "        float: Estimated noise scale.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not grad_list:\n",
    "            return None\n",
    "\n",
    "        # Stack gradients: shape (num_batches, num_params)\n",
    "        grad_stack = torch.stack(grad_list)\n",
    "        mean_grad = grad_stack.mean(dim=0)\n",
    "        # Compute average variance per parameter element.\n",
    "        var_grad = grad_stack.var(dim=0, unbiased=False).mean()\n",
    "        denom = mean_grad.norm()**2 + eps\n",
    "        noise_scale = var_grad / denom\n",
    "        return noise_scale.item()\n",
    "    except Exception as e:\n",
    "        log(DEBUG, \"Error in compute_noise_scale_from_gradients: %s\", e)\n",
    "        return None\n",
    "\n",
    "def get_gradient_vector(model, data, target, loss_fn, device):\n",
    "    model.zero_grad()\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output = model(data)\n",
    "    loss = loss_fn(output, target)\n",
    "    loss.backward()\n",
    "    \n",
    "    grads = []\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            grads.append(p.grad.view(-1))\n",
    "    if grads:\n",
    "        return torch.cat(grads)\n",
    "    log(DEBUG, \"No gradients found\")\n",
    "    return None\n",
    "\n",
    "def collect_gradients(model, train_loader, device, criterion, num_mini_batches):\n",
    "    grad_vectors = []\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        if i >= num_mini_batches:\n",
    "            break\n",
    "        grad_vector = get_gradient_vector(model, data, target, criterion, device)\n",
    "        if grad_vector is not None:\n",
    "            grad_vectors.append(grad_vector)\n",
    "    return grad_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerRayClient(flwr.client.NumPyClient):\n",
    "    \"\"\"Flower client for the FEMNIST dataset.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cid: int,\n",
    "        partition_dir: Path,\n",
    "        model_generator: Callable[[], Module],\n",
    "    ) -> None:\n",
    "        \"\"\"Init the client with its unique id and the folder to load data from.\n",
    "\n",
    "        Parameters:\n",
    "            cid (int): Unique client id for a client used to map it to its data\n",
    "                partition\n",
    "            partition_dir (Path): The directory containing data for each\n",
    "                client/client id\n",
    "            model_generator (Callable[[], Module]): The model generator function\n",
    "        \n",
    "        \"\"\"\n",
    "        self.cid = cid\n",
    "        log(INFO, \"cid: %s\", self.cid)\n",
    "        self.partition_dir = partition_dir\n",
    "        self.device = str(\n",
    "            torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        )\n",
    "        self.model_generator: Callable[[], Module] = model_generator\n",
    "        self.properties: dict[str, Scalar] = {\"tensor_type\": \"numpy.ndarray\"}\n",
    "\n",
    "    def set_parameters(self, parameters: NDArrays) -> Module:\n",
    "        \"\"\"Load weights inside the network.\"\"\"\n",
    "        net = self.model_generator()\n",
    "        return set_model_parameters(net, parameters)\n",
    "\n",
    "    def get_parameters(self, config: dict[str, Scalar]) -> NDArrays:\n",
    "        \"\"\"Return weights from a given model.\n",
    "\n",
    "        If no model is passed, then a local model is created.\n",
    "        This can be used to initialise a model in the\n",
    "        server.\n",
    "        The config param is not used but is mandatory in Flower.\n",
    "\n",
    "        \"\"\"\n",
    "        net = self.model_generator()\n",
    "        return get_model_parameters(net)\n",
    "\n",
    "    def fit(self, parameters: NDArrays, config: dict[str, Scalar]) -> tuple[NDArrays, int, dict]:\n",
    "        \"\"\"Receive and train a model on the local client data.\"\"\"\n",
    "        # Only create model right before training/testing\n",
    "        # To lower memory usage when idle\n",
    "        try:\n",
    "            net = self.set_parameters(parameters)\n",
    "            net.to(self.device)\n",
    "\n",
    "\n",
    "            train_loader: DataLoader = self._create_data_loader(config, name=\"train\")\n",
    "            if len(train_loader) == 0:\n",
    "                log(INFO, f\"---------------------- B train loader: {self.cid}, len: {len(train_loader)}\")\n",
    "            train_loss = self._train(net, train_loader=train_loader, config=config)\n",
    "\n",
    "            # Compute gradients\n",
    "            # Collect gradients for noise scale estimation.\n",
    "            grad_vectors = collect_gradients(net, train_loader, self.device, torch.nn.CrossEntropyLoss(), 5)\n",
    "            # Compute local noise scale (Bsimple) on this client.\n",
    "            local_noise_scale = compute_noise_scale_from_gradients(grad_vectors)\n",
    "            return get_model_parameters(net), len(train_loader), {\"train_loss\": train_loss, \"noise_scale\": local_noise_scale}\n",
    "        except Exception as e:\n",
    "            log(DEBUG, f\"---------------------- A client raised error: {e}: {self.cid}\")\n",
    "\n",
    "    def evaluate(self, parameters: NDArrays, config: dict[str, Scalar]) -> tuple[float, int, dict]:\n",
    "        \"\"\"Receive and test a model on the local client data.\"\"\"\n",
    "        net = self.set_parameters(parameters)\n",
    "        net.to(self.device)\n",
    "\n",
    "        test_loader: DataLoader = self._create_data_loader(config, name=\"test\")\n",
    "        loss, accuracy = self._test(net, test_loader=test_loader, config=config)\n",
    "        return loss, len(test_loader), {\"local_accuracy\": accuracy}\n",
    "\n",
    "    def _create_data_loader(self, config: dict[str, Scalar], name: str) -> DataLoader:\n",
    "        \"\"\"Create the data loader using the specified config parameters.\"\"\"\n",
    "        batch_size = int(config[\"batch_size\"])\n",
    "        num_workers = int(config[\"num_workers\"])\n",
    "        dataset = self._load_dataset(name)\n",
    "        if len(dataset) == 0:\n",
    "            log(INFO, f\"---------------------- B train dataset: {self.cid}, len: {len(dataset)}, batch size: {batch_size}\")\n",
    "        else:\n",
    "            log(INFO, f\"---------------------- C train dataset: {self.cid}, len: {len(dataset)}, batch size: {batch_size}\")\n",
    "        return DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=(name == \"train\"),\n",
    "        )\n",
    "\n",
    "    def _load_dataset(self, name: str) -> Dataset:\n",
    "        full_file: Path = self.partition_dir / str(self.cid)\n",
    "        return load_femnist_dataset(\n",
    "            mapping=full_file,\n",
    "            name=name,\n",
    "            data_dir=data_dir,\n",
    "        )\n",
    "\n",
    "    def _train(\n",
    "        self, net: Module, train_loader: DataLoader, config: dict[str, Scalar]\n",
    "    ) -> float:\n",
    "        return train_femnist(\n",
    "            net=net,\n",
    "            train_loader=train_loader,\n",
    "            epochs=int(config[\"epochs\"]),\n",
    "            device=self.device,\n",
    "            optimizer=torch.optim.AdamW(\n",
    "                net.parameters(),\n",
    "                lr=float(config[\"client_learning_rate\"]),\n",
    "                weight_decay=float(config[\"weight_decay\"]),\n",
    "            ),\n",
    "            criterion=torch.nn.CrossEntropyLoss(),\n",
    "            max_batches=int(config[\"max_batches\"]),\n",
    "            cid=self.cid,\n",
    "        )\n",
    "\n",
    "    def _test(\n",
    "        self, net: Module, test_loader: DataLoader, config: dict[str, Scalar]\n",
    "    ) -> tuple[float, float]:\n",
    "        return test_femnist(\n",
    "            net=net,\n",
    "            test_loader=test_loader,\n",
    "            device=self.device,\n",
    "            criterion=torch.nn.CrossEntropyLoss(),\n",
    "            max_batches=int(config[\"max_batches\"]),\n",
    "        )\n",
    "\n",
    "    def get_properties(self, config: dict[str, Scalar]) -> dict[str, Scalar]:\n",
    "        \"\"\"Return properties for this client.\"\"\"\n",
    "        return self.properties\n",
    "\n",
    "    def get_train_set_size(self) -> int:\n",
    "        \"\"\"Return the client train set size.\"\"\"\n",
    "        return len(self._load_dataset(\"train\"))  # type: ignore[reportArgumentType]\n",
    "\n",
    "    def get_test_set_size(self) -> int:\n",
    "        \"\"\"Return the client test set size.\"\"\"\n",
    "        return len(self._load_dataset(\"test\"))  # type: ignore[reportArgumentType]\n",
    "\n",
    "\n",
    "def fit_client_seeded(\n",
    "    client: FlowerRayClient,\n",
    "    params: NDArrays,\n",
    "    conf: dict[str, Any],\n",
    "    seed: Seeds = Seeds.DEFAULT,\n",
    "    **kwargs: Any,\n",
    ") -> tuple[NDArrays, int, dict]:\n",
    "    \"\"\"Wrap to always seed client training.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    return client.fit(params, conf, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flower_client_generator(\n",
    "    model_generator: Callable[[], Module],\n",
    "    partition_dir: Path,\n",
    "    mapping_fn: Callable[[int], int] | None = None,\n",
    ") -> Callable[[str], FlowerRayClient]:\n",
    "    \"\"\"Wrap the client instance generator.\n",
    "\n",
    "    A mapping function could be used for filtering/ordering clients.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        model_generator (Callable[[], Module]): model generator function.\n",
    "        partition_dir (Path): directory containing the partition.\n",
    "        mapping_fn (Optional[Callable[[int], int]]): function mapping sorted/filtered\n",
    "            ids to real cid.\n",
    "    \"\"\"\n",
    "\n",
    "    def client_fn(cid: str) -> FlowerRayClient:\n",
    "        \"\"\"Create a single client instance given the client id `cid`.\"\"\"\n",
    "        return FlowerRayClient(\n",
    "            cid=mapping_fn(int(cid)) if mapping_fn is not None else int(cid),\n",
    "            partition_dir=partition_dir,\n",
    "            model_generator=model_generator,\n",
    "        )\n",
    "\n",
    "    return client_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_clients(\n",
    "    total_clients: int,\n",
    "    filter_less: int,\n",
    "    partition: Path,\n",
    "    seed: int | None = Seeds.DEFAULT,\n",
    ") -> Sequence[int]:\n",
    "    \"\"\"Sample randomly clients.\n",
    "\n",
    "    A filter on the client train set size is performed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        total_clients (int): total number of clients to sample.\n",
    "        filter_less (int): max number of train samples for which the client is\n",
    "            **discarded**.\n",
    "        partition (Path): path to the folder containing the partitioning.\n",
    "    \"\"\"\n",
    "    real_federated_cid_client_generator: Callable[[str], FlowerRayClient] = (\n",
    "        get_flower_client_generator(network_generator, federated_partition)\n",
    "    )\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    list_of_ids = []\n",
    "    while len(list_of_ids) < total_clients:\n",
    "        current_id = random.randint(0, 3229)\n",
    "        if (\n",
    "            real_federated_cid_client_generator(str(current_id)).get_train_set_size()\n",
    "            > filter_less\n",
    "        ):\n",
    "            list_of_ids.append(current_id)\n",
    "    return list_of_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_federated_evaluation_function(\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    "    model_generator: Callable[[], Module],\n",
    "    criterion: Module,\n",
    "    max_batches: int,\n",
    ") -> Callable[[int, NDArrays, dict[str, Any]], tuple[float, dict[str, Scalar]]]:\n",
    "    \"\"\"Wrap the external federated evaluation function.\n",
    "\n",
    "    It provides the external federated evaluation function with some\n",
    "    parameters for the dataloader, the model generator function, and\n",
    "    the criterion used in the evaluation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        batch_size (int): batch size of the test set to use.\n",
    "        num_workers (int): correspond to `num_workers` param in the Dataloader object.\n",
    "        model_generator (Callable[[], Module]):  model generator function.\n",
    "        criterion (Module): PyTorch Module containing the criterion for evaluating the\n",
    "        model.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        External federated evaluation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def federated_evaluation_function(\n",
    "        server_round: int,\n",
    "        parameters: NDArrays,\n",
    "        fed_eval_config: dict[\n",
    "            str, Any\n",
    "        ],  # mandatory argument, even if it's not being used\n",
    "    ) -> tuple[float, dict[str, Scalar]]:\n",
    "        \"\"\"Evaluate federated model on the server.\n",
    "\n",
    "        It uses the centralized val set for sake of simplicity.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            server_round (int): current federated round.\n",
    "            parameters (NDArrays): current model parameters.\n",
    "            fed_eval_config (dict[str, Any]): mandatory argument in Flower, can contain\n",
    "                some configuration info\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            tuple[float, dict[str, Scalar]]: evaluation results\n",
    "        \"\"\"\n",
    "        device: str = get_device()\n",
    "        net: Module = set_model_parameters(model_generator(), parameters)\n",
    "        net.to(device)\n",
    "\n",
    "        full_file: Path = centralized_mapping\n",
    "        dataset: Dataset = load_femnist_dataset(data_dir, full_file, \"val\")\n",
    "\n",
    "        valid_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "        loss, acc = test_femnist(\n",
    "            net=net,\n",
    "            test_loader=valid_loader,\n",
    "            device=device,\n",
    "            criterion=criterion,\n",
    "            max_batches=max_batches,\n",
    "        )\n",
    "        return loss, {\"accuracy\": acc}\n",
    "\n",
    "    return federated_evaluation_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_weighted_average(metrics: list[tuple[int, dict]]) -> dict:\n",
    "    \"\"\"Combine results from multiple clients following training or evaluation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        metrics (list[tuple[int, dict]]): collected clients metrics\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        dict: result dictionary containing the aggregate of the metrics passed.\n",
    "    \"\"\"\n",
    "    average_dict: dict = defaultdict(list)\n",
    "    total_examples: int = 0\n",
    "    for num_examples, metrics_dict in metrics:\n",
    "        for key, val in metrics_dict.items():\n",
    "            if isinstance(val, numbers.Number):\n",
    "                average_dict[key].append((num_examples, val))\n",
    "        total_examples += num_examples\n",
    "    return {\n",
    "        key: {\n",
    "            \"avg\": float(\n",
    "                sum([num_examples * metric for num_examples, metric in val])\n",
    "                / float(total_examples)\n",
    "            ),\n",
    "            \"all\": val,\n",
    "        }\n",
    "        for key, val in average_dict.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_seeded_simulation(\n",
    "    client_fn: Callable[[str], Client],\n",
    "    num_clients: int,\n",
    "    config: ServerConfig,\n",
    "    strategy: Strategy,\n",
    "    name: str,\n",
    "    return_all_parameters: bool = False,\n",
    "    seed: int = Seeds.DEFAULT,\n",
    "    iteration: int = 0,\n",
    ") -> tuple[list[tuple[int, NDArrays]], History]:\n",
    "    \"\"\"Wrap to seed client selection.\"\"\"\n",
    "    np.random.seed(seed ^ iteration)\n",
    "    torch.manual_seed(seed ^ iteration)\n",
    "    random.seed(seed ^ iteration)\n",
    "    parameter_list, hist = flwr.simulation.start_simulation_no_ray(\n",
    "        client_fn=client_fn,\n",
    "        num_clients=num_clients,\n",
    "        client_resources={},\n",
    "        config=config,\n",
    "        strategy=strategy,\n",
    "    )\n",
    "    save_history(home_dir, hist, name)\n",
    "    return parameter_list, hist\n",
    "\n",
    "def run_simulation(\n",
    "    num_rounds: int,\n",
    "    num_total_clients: int,\n",
    "    num_clients_per_round: int,\n",
    "    num_evaluate_clients: int,\n",
    "    min_available_clients: int,\n",
    "    min_fit_clients: int,\n",
    "    min_evaluate_clients: int,\n",
    "    evaluate_fn: (Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None),\n",
    "    on_fit_config_fn: Callable[[int], dict[str, Scalar]],\n",
    "    on_evaluate_config_fn: Callable[[int], dict[str, Scalar]],\n",
    "    initial_parameters: Parameters,\n",
    "    fit_metrics_aggregation_fn: Callable | None,\n",
    "    evaluate_metrics_aggregation_fn: Callable | None,\n",
    "    federated_client_generator: Callable[[str], flwr.client.NumPyClient],\n",
    "    server_learning_rate: float = 1.0,\n",
    "    server_momentum: float = 0.0,\n",
    "    accept_failures: bool = False,\n",
    ") -> tuple[list[tuple[int, NDArrays]], History]:\n",
    "    \"\"\"Run a federated simulation using Flower.\"\"\"\n",
    "    log(INFO, \"FL will execute for %s rounds\", num_rounds)\n",
    "\n",
    "    # Percentage of clients used for train/eval\n",
    "    fraction_fit: float = float(num_clients_per_round) / num_total_clients\n",
    "    fraction_evaluate: float = float(num_evaluate_clients) / num_total_clients\n",
    "\n",
    "    strategy = FedAvg(\n",
    "        fraction_fit=fraction_fit,\n",
    "        fraction_evaluate=fraction_evaluate,\n",
    "        min_fit_clients=min_fit_clients,\n",
    "        min_evaluate_clients=min_evaluate_clients,\n",
    "        min_available_clients=min_available_clients,\n",
    "        on_fit_config_fn=on_fit_config_fn,\n",
    "        on_evaluate_config_fn=on_evaluate_config_fn,\n",
    "        evaluate_fn=evaluate_fn,\n",
    "        initial_parameters=initial_parameters,\n",
    "        accept_failures=accept_failures,\n",
    "        fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n",
    "        evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n",
    "        server_learning_rate=server_learning_rate,\n",
    "        server_momentum=server_momentum,\n",
    "    )\n",
    "    # resetting the seed for the random selection of clients\n",
    "    # this way the list of clients trained is guaranteed to be always the same\n",
    "\n",
    "    cfg = ServerConfig(num_rounds)\n",
    "\n",
    "    def simulator_client_generator(cid: str) -> Client:\n",
    "        return federated_client_generator(cid).to_client()\n",
    "\n",
    "    parameters_for_each_round, hist = start_seeded_simulation(\n",
    "        client_fn=simulator_client_generator,\n",
    "        num_clients=num_total_clients,\n",
    "        config=cfg,\n",
    "        strategy=strategy,\n",
    "        name=\"fedavg\",\n",
    "        return_all_parameters=True,\n",
    "        seed=Seeds.DEFAULT,\n",
    "    )\n",
    "    return parameters_for_each_round, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_critical_batch(noise_scales: list, constant: float = 1.0) -> float:\n",
    "    # simple avg of noise scales\n",
    "    avg_noise_scale = np.mean(noise_scales)\n",
    "    eps = 1e-8\n",
    "    \n",
    "    # Computing an estimated critical batch size (Bcrit) using a simple heuristic.\n",
    "    critical_batch_size = constant / (avg_noise_scale + eps)\n",
    "    return critical_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centralized_acc_from_hist(hist):\n",
    "    accuracies = [val for r, val in hist.metrics_centralized['accuracy']]\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_generator = get_network_generator()\n",
    "seed_net = network_generator()\n",
    "seed_model_params = get_model_parameters(seed_net)\n",
    "\n",
    "# Federated configuration dictionary\n",
    "federated_train_config = {\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 32,\n",
    "    \"client_learning_rate\": 0.01,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"num_workers\": 0,\n",
    "    \"max_batches\": 100,\n",
    "}\n",
    "\n",
    "federated_test_config: dict[str, Any] = {\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 0,\n",
    "    \"max_batches\": 100,\n",
    "}\n",
    "\n",
    "num_rounds = 10\n",
    "num_total_clients = 100\n",
    "num_evaluate_clients = 0\n",
    "num_clients_per_round = 10\n",
    "\n",
    "initial_parameters = ndarrays_to_parameters(seed_model_params)\n",
    "\n",
    "federated_evaluation_function = get_federated_evaluation_function(\n",
    "    batch_size=federated_test_config[\"batch_size\"],\n",
    "    num_workers=federated_test_config[\"num_workers\"],\n",
    "    model_generator=network_generator,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    max_batches=federated_test_config[\"max_batches\"],\n",
    ")\n",
    "\n",
    "server_learning_rate = 1.0\n",
    "server_momentum = 0.0\n",
    "accept_failures = False\n",
    "\n",
    "list_of_ids = sample_random_clients(\n",
    "    num_total_clients, federated_train_config[\"batch_size\"], federated_partition\n",
    ")\n",
    "\n",
    "federated_client_generator = (\n",
    "    get_flower_client_generator(\n",
    "        network_generator, federated_partition, lambda seq_id: list_of_ids[seq_id]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_batch_sizes = [8, 16, 32, 64, 128]\n",
    "total_batches_results = []\n",
    "\n",
    "for batch_size in experiment_batch_sizes:\n",
    "    train_cfg = federated_train_config.copy()\n",
    "    train_cfg[\"batch_size\"] = batch_size\n",
    "\n",
    "    test_cfg = federated_test_config.copy()\n",
    "    test_cfg[\"batch_size\"] = batch_size\n",
    "\n",
    "    local_list_of_ids = sample_random_clients(\n",
    "    num_total_clients, train_cfg[\"batch_size\"], federated_partition\n",
    "    )\n",
    "\n",
    "    local_federated_client_generator = (\n",
    "        get_flower_client_generator(\n",
    "            network_generator, federated_partition, lambda seq_id: local_list_of_ids[seq_id]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    parameters_for_each_round, hist = run_simulation(\n",
    "        num_rounds = num_rounds,\n",
    "        num_total_clients = num_total_clients,\n",
    "        num_clients_per_round = num_clients_per_round,\n",
    "        num_evaluate_clients = num_evaluate_clients,\n",
    "        min_available_clients = num_total_clients,\n",
    "        min_fit_clients = num_clients_per_round,\n",
    "        min_evaluate_clients = num_evaluate_clients,\n",
    "        evaluate_fn = federated_evaluation_function,\n",
    "        on_fit_config_fn = lambda _: train_cfg,\n",
    "        on_evaluate_config_fn = lambda _: test_cfg,\n",
    "        initial_parameters = initial_parameters,\n",
    "        fit_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        evaluate_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        federated_client_generator = local_federated_client_generator,\n",
    "        server_learning_rate=server_learning_rate,\n",
    "        server_momentum=server_momentum,\n",
    "        accept_failures=accept_failures,\n",
    "        )\n",
    "\n",
    "    total_batches_results.append((batch_size, parameters_for_each_round, hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "# Left subplot: Accuracy vs Rounds for each batch size\n",
    "for batch_size, params, hist in total_batches_results:\n",
    "    accuracies = get_centralized_acc_from_hist(hist)\n",
    "    axes[0].plot(accuracies, label=f\"Batch Size: {batch_size}\")\n",
    "axes[0].set_xlabel(\"Round\")\n",
    "axes[0].set_ylabel(\"Accuracy\")\n",
    "axes[0].set_title(\"Federated Training: Accuracy vs Rounds\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Right subplot: Max Accuracy vs Batch Size\n",
    "batch_sizes = []\n",
    "max_accuracies = []\n",
    "for batch_size, params, hist in total_batches_results:\n",
    "    accuracies = get_centralized_acc_from_hist(hist)\n",
    "    if accuracies:\n",
    "        batch_sizes.append(batch_size)\n",
    "        max_accuracies.append(max(accuracies))\n",
    "axes[1].plot(batch_sizes, max_accuracies, marker='o')\n",
    "axes[1].set_xlabel(\"Batch Size\")\n",
    "axes[1].set_ylabel(\"Max Accuracy\")\n",
    "axes[1].set_title(\"Max Accuracy vs Local Batch Size\")\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_size, params, hist in total_batches_results:\n",
    "    print(\"Batch size: \", batch_size)\n",
    "\n",
    "    noise_scales = hist.metrics_distributed_fit['noise_scale']\n",
    "\n",
    "\n",
    "    crit_batches = []\n",
    "    for round, round_noise_scales in noise_scales:\n",
    "        actual_noise_scales = [val for _, val in round_noise_scales['all']]\n",
    "        crit_batch = compute_critical_batch(actual_noise_scales, .001)\n",
    "        crit_batches.append(crit_batch)\n",
    "    print(\"---\", crit_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# client cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_sizes = [5, 10, 20, 50, 75]\n",
    "total_cohort_results = []\n",
    "\n",
    "for cohort_size in cohort_sizes:\n",
    "    train_cfg = federated_train_config.copy()\n",
    "    test_cfg = federated_test_config.copy()\n",
    "\n",
    "    parameters_for_each_round, hist = run_simulation(\n",
    "        num_rounds = num_rounds,\n",
    "        num_total_clients = num_total_clients,\n",
    "        num_clients_per_round = cohort_size,\n",
    "        num_evaluate_clients = num_evaluate_clients,\n",
    "        min_available_clients = num_total_clients,\n",
    "        min_fit_clients = cohort_size,\n",
    "        min_evaluate_clients = num_evaluate_clients,\n",
    "        evaluate_fn = federated_evaluation_function,\n",
    "        on_fit_config_fn = lambda _: train_cfg,\n",
    "        on_evaluate_config_fn = lambda _: test_cfg,\n",
    "        initial_parameters = initial_parameters,\n",
    "        fit_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        evaluate_metrics_aggregation_fn = aggregate_weighted_average,\n",
    "        federated_client_generator = federated_client_generator,\n",
    "        server_learning_rate=server_learning_rate,\n",
    "        server_momentum=server_momentum,\n",
    "        accept_failures=accept_failures,\n",
    "        )\n",
    "\n",
    "    total_cohort_results.append((cohort_size, parameters_for_each_round, hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "# Left subplot: Accuracy vs Rounds for each client cohort size\n",
    "for cohort_size, params, hist in total_cohort_results:\n",
    "    accuracies = get_centralized_acc_from_hist(hist)\n",
    "    axes[0].plot(accuracies, label=f\"Client Cohort Size: {cohort_size}\")\n",
    "axes[0].set_xlabel(\"Round\")\n",
    "axes[0].set_ylabel(\"Accuracy\")\n",
    "axes[0].set_title(\"Federated Training: Accuracy vs Rounds (Varying Cohort Sizes)\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Right subplot: Max Accuracy vs Client Cohort Size\n",
    "cohort_sizes = []\n",
    "max_accuracies = []\n",
    "for cohort_size, params, hist in total_cohort_results:\n",
    "    accuracies = get_centralized_acc_from_hist(hist)\n",
    "    if accuracies:\n",
    "        cohort_sizes.append(cohort_size)\n",
    "        max_accuracies.append(max(accuracies))\n",
    "axes[1].plot(cohort_sizes, max_accuracies, marker='o')\n",
    "axes[1].set_xlabel(\"Client Cohort Size\")\n",
    "axes[1].set_ylabel(\"Max Accuracy\")\n",
    "axes[1].set_title(\"Max Accuracy vs Client Cohort Size\")\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort_size, params, hist in total_cohort_results:\n",
    "    print(\"Cohort size: \", cohort_size)\n",
    "\n",
    "    noise_scales = hist.metrics_distributed_fit['noise_scale']\n",
    "\n",
    "    crit_batches = []\n",
    "    for round, round_noise_scales in noise_scales:\n",
    "        actual_noise_scales = [val for _, val in round_noise_scales['all']]\n",
    "        crit_batch = compute_critical_batch(actual_noise_scales, .001)\n",
    "        crit_batches.append(crit_batch)\n",
    "    print(\"---\", crit_batches)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('total_cohort_results.json', 'w') as f:\n",
    "    json.dump(total_cohort_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('total_cohort_results.json', 'r') as f:\n",
    "    total_cohort_results = json.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
